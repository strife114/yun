# 基本环境搭建

## 修改主机名(2)

```sh
hostnamectl set-hostname controller
bash

hostnamectl set-hostname compute
bash
```

## yum源搭建（2）

1. 挂载光盘

   ```sh
   【挂载CentOS-7-x86_64-DVD-2009.iso】
   [root@controller ~]# mount -o loop CentOS-7-x86_64-DVD-2009.iso   /mnt/
   [root@controller ~]# mkdir /opt/centos /opt/iaas
   [root@controller ~]# cp -rvf /mnt/* /opt/centos/
   [root@controller ~]# umount  /mnt/
   
   【挂载chinaskills_cloud_iaas_v2.0.3.is】
   [root@controller ~]# mount -o loop chinaskills_cloud_iaas_v2.0.3.iso /mnt/
   [root@controller ~]# cp -rvf /mnt/* /opt/iaas
   [root@controller ~]# umount  /mnt/
   
   
   
   
   
   # 查看yum源
   yum repolist
   ```
   
2. 创建repo文件

   ```sh
   删除所有的yum配置文件
   rm -rf /etc/yum.repo.d/*
   
   # 创建centos.repo文件
   # 注意隐藏字符，容易导致报错
   [centos]
   name=centos
   baseurl=file:///opt/centos
   gpgcheck=0
   enabled=1
   [iaas]
   name=iaas
   baseurl=file:///opt/iaas/iaas-repo
   gpgcheck=0
   enabled=1
   
   
   
   ```

3. 执行yum源更新命令

   ```
   yum clean all
   yum makecache 或 yum list  #本质都是生成缓存
   ```
   
4. 安装vim、net-tools

   ```
   yum install -y vim
   
   yum install -y net-tools
   netstat -ntlp  //查看是否安装成功
   ```
   
## 配置安全（selinux和防火墙、2）

  ```sh
   vi /etc/selinux/config
   SELINUX=disabled
   
   systemctl stop firewalld.service
   systemctl disable firewalld.service
   yum remove -y NetworkManager firewalld
   
   
   # 安装iptables，清空防火墙规则
   yum install -y iptables-services
   systemctl enable iptables
   systemctl restart iptables
   iptables -F
   iptables -X
   iptables -Z
   service iptables save
  
  
  
  
  ```

## 设置密钥传输

```sh
# vi /etc/hosts
192.168.223.120 controller
192.168.223.121 compute

# ssh-keygen
回车
# ssh-copy-id compute


# 概率出现检测不到公钥
rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7
```



## 编辑环境变量（2）

1. 下载iaas-xiandian

   ```sh
   yum install -y openstack-iaas
   ```

2. 编辑执行文件

   ```sh
   root@controller ~]# cat /etc/openstack/openrc.sh 
   #--------------------system Config--------------------##
   #Controller Server Manager IP. example:x.x.x.x
   HOST_IP=192.168.223.20
   
   #Controller HOST Password. example:000000 
   HOST_PASS=000000000000
   
   #Controller Server hostname. example:controller
   HOST_NAME=controller
   
   #Compute Node Manager IP. example:x.x.x.x
   HOST_IP_NODE=192.168.223.21
   
   #Compute HOST Password. example:000000 
   HOST_PASS_NODE=000000
   
   #Compute Node hostname. example:compute
   HOST_NAME_NODE=compute
   
   #--------------------Chrony Config-------------------##
   #Controller network segment IP.  example:x.x.0.0/16(x.x.x.0/24)
   # 网段
   network_segment_IP=192.168.223.0/24
   
   #--------------------Rabbit Config ------------------##
   #user for rabbit. example:openstack
   RABBIT_USER=openstack
   
   #Password for rabbit user .example:000000
   RABBIT_PASS=000000
   
   #--------------------MySQL Config---------------------##
   #Password for MySQL root user . exmaple:000000
   DB_PASS=000000
   
   #--------------------Keystone Config------------------##
   #Password for Keystore admin user. exmaple:000000
   DOMAIN_NAME=demo
   ADMIN_PASS=000000
   DEMO_PASS=000000
   
   #Password for Mysql keystore user. exmaple:000000
   KEYSTONE_DBPASS=000000
   
   #--------------------Glance Config--------------------##
   #Password for Mysql glance user. exmaple:000000
   GLANCE_DBPASS=000000
   
   #Password for Keystore glance user. exmaple:000000
   GLANCE_PASS=000000
   
   #--------------------Placement Config----------------------##
   #Password for Mysql placement user. exmaple:000000
   PLACEMENT_DBPASS=000000
   
   #Password for Keystore placement user. exmaple:000000
   PLACEMENT_PASS=000000
   
   #--------------------Nova Config----------------------##
   #Password for Mysql nova user. exmaple:000000
   NOVA_DBPASS=000000
   
   #Password for Keystore nova user. exmaple:000000
   NOVA_PASS=000000
   
   #--------------------Neutron Config-------------------##
   #Password for Mysql neutron user. exmaple:000000
   NEUTRON_DBPASS=000000
   
   #Password for Keystore neutron user. exmaple:000000
   NEUTRON_PASS=000000
   
   #metadata secret for neutron. exmaple:000000
   METADATA_SECRET=000000
   
   #External Network Interface. example:eth1
   # 第二块网卡名称
   INTERFACE_NAME=ens34
   
   #External Network The Physical Adapter. example:provider
   Physical_NAME=provider
   
   #First Vlan ID in VLAN RANGE for VLAN Network. exmaple:101
   # vlan初始范围
   minvlan=1
   
   #Last Vlan ID in VLAN RANGE for VLAN Network. example:200
   # vlan的结束范围
   maxvlan=1000
   
   #--------------------Cinder Config--------------------##
   #Password for Mysql cinder user. exmaple:000000
   CINDER_DBPASS=000000
   
   #Password for Keystore cinder user. exmaple:000000
   CINDER_PASS=000000
   
   #Cinder Block Disk. example:md126p3
   # 自己分区的空白分区2
   BLOCK_DISK=sdb1
   
   #--------------------Swift Config---------------------##
   #Password for Keystore swift user. exmaple:000000
   SWIFT_PASS=000000
   
   #The NODE Object Disk for Swift. example:md126p4.
   # 自己分区的空白分区2
   OBJECT_DISK=sdb2
   
   #The NODE IP for Swift Storage Network. example:x.x.x.x.
   # compute节点ip
   STORAGE_LOCAL_NET_IP=192.168.223.21
   
   #--------------------Trove Config----------------------##
   #Password for Mysql trove user. exmaple:000000
   TROVE_DBPASS=000000
   
   #Password for Keystore trove user. exmaple:000000
   TROVE_PASS=000000
   
   #--------------------Heat Config----------------------##
   #Password for Mysql heat user. exmaple:000000
   HEAT_DBPASS=000000
   
   #Password for Keystore heat user. exmaple:000000
   HEAT_PASS=000000
   
   #--------------------Ceilometer Config----------------##
   #Password for Gnocchi ceilometer user. exmaple:000000
   CEILOMETER_DBPASS=000000
   
   #Password for Keystore ceilometer user. exmaple:000000
   CEILOMETER_PASS=000000
   
   #--------------------AODH Config----------------##
   #Password for Mysql AODH user. exmaple:000000
   AODH_DBPASS=000000
   
   #Password for Keystore AODH user. exmaple:000000
   AODH_PASS=000000
   
   #--------------------ZUN Config----------------##
   #Password for Mysql ZUN user. exmaple:000000
   ZUN_DBPASS=000000
   
   #Password for Keystore ZUN user. exmaple:000000
   ZUN_PASS=000000
   
   #Password for Keystore KURYR user. exmaple:000000
   KURYR_PASS=000000
   
   #--------------------OCTAVIA Config----------------##
   #Password for Mysql OCTAVIA user. exmaple:000000
   OCTAVIA_DBPASS=000000
   
   #Password for Keystore OCTAVIA user. exmaple:000000
   OCTAVIA_PASS=000000
   
   #--------------------Manila Config----------------##
   #Password for Mysql Manila user. exmaple:000000
   MANILA_DBPASS=000000
   
   #Password for Keystore Manila user. exmaple:000000
   MANILA_PASS=000000
   
   #The NODE Object Disk for Manila. example:md126p5.
   SHARE_DISK=sdb3
   
   #--------------------Cloudkitty Config----------------##
   #Password for Mysql Cloudkitty user. exmaple:000000
   CLOUDKITTY_DBPASS=000000
   
   #Password for Keystore Cloudkitty user. exmaple:000000
   CLOUDKITTY_PASS=000000
   
   #--------------------Barbican Config----------------##
   #Password for Mysql Barbican user. exmaple:000000
   BARBICAN_DBPASS=000000
   
   #Password for Keystore Barbican user. exmaple:000000
   BARBICAN_PASS=000000
   ###############################################################
   #####在vi编辑器中执行:%s/^.\{1\}//  删除每行前1个字符(#号)#####
   ###############################################################
   
   ```







## 安装Openstack(2)

### 简介

1. OpenStack既是一个社区，也是一个项目和一个开源软件，它提供了一个部署云的操作平台或工具集（IaaS）。其宗旨在于：帮助组织运行为虚拟计算或存储服务的云，为公有云、私有云提供可扩展的、灵活的云计算
2. OpenStack主要包含以下几个组件，各个组件的功能如下，其中Nova，Keyston，Neutron，Glance，DashBoard为必须装的组件，其余可以选择性安装，图为OpenStack生态系统





```sh
# 基础配置命令(安装Openstack、chrony、配置域名)己编写成shell脚本，通过脚本一键安装
# 注意重启
iaas-pre-host.sh
```





## 安装数据库(controller)

1. 安装

   ```sh
   # 在controller节点上使用iaas-install-mysql.sh 脚本安装Mariadb、Memcached、RabbitMQ消息队列等服务
   iaas-install-mysql.sh
   ```
   
2. 配置数据库

   ```sh
   # 创建test库
   create database test;
   use test;
   
   # 在库test中创建表company（表结构如(id int not null primary key,name varchar(50),addr varchar(255))
   create table company(id int(10) not null primary key, name varchar(50), addr varchar(255));
   
   # 在表company中插入一条数据(1,"alibaba","china")
   insert into company(id,name,addr)values(1,"alibaaba","china");
   
   flush privileges;
   ```

3. 配置RabbitMQ服务

   ```sh
   # 创建用户chinaskill，密码为chinapd
   rabbitmqctl add_user chinaskill chinapd
   # 赋予administrator权限
   rabbitmqctl set_user_tags chinaskill administrator
   ```



## 安装Keystone认证服务(controller)

### 简介

1. 提供所有组件的认证

### 安装

1. 安装

   ```sh
   # 使用脚本iaas-install-keystone.sh安装Keystone服务
   iaas-install-keystone.sh
   ```



## 安装Glance镜像服务（controller）

### 简介

1. 提供镜像服务

### 安装

1. 安装

   ```sh
   # 使用iaas-install-glance.sh脚本安装glance 服务
   iaas-install-glance.sh
   ```




## 安装Placemen统计资源服务（controller）

### 简介

Placement】服务 是从【nova】服务中拆分出来的组件，作用是收集各个【node】节点的可用资源，把【node】节点的资源统计写入到【MySQL】

### 安装

1. 安装

   ```sh
    iaas-install-placement.sh
   ```

   

## 安装Nova计算服务(2)

### 简介

1. 计算管理服务，支撑虚拟机运行

### 安装

1. 安装

   ```sh
   # 执行脚本部署nova组件的控制服务
   iaas-install-nova-controller.sh
   
   # 执行完上面的脚本后，在compute节点执行脚本部署nova组件的计算服务，这样就将compute节点的cpu、内存及磁盘资源添加到OpenStack云平台的资源池中了。
   iaas-install-nova-compute.sh
   ```



## 安装Neutron网络服务(2)

### 简介

1. 提供网络支持

### 安装

1. 安装

   ```sh
   # 分别iaas-install-neutron-controller.sh脚本、iaas-install-neutron-compute.sh脚本分别安装 Neutron 服务，网络默认是vlan模式
   iaas-install-neutron-controller.sh
   iaas-install-neutron-compute.sh
   ```



## 安装Dashboard网页管理界面服务(controller)

### 简介

1. 提供web管理界面服务

### 安装

1. 安装

   ```sh
   # 使用iaas-install-dashboard.sh脚本安装dashboad服务。
   iaas-install-dashboard.sh
   
   
   # 到此步可截至，接下来的根据比赛要求而定
   ```



## 安装Swift对象存储服务(2)

### 简介

1. 提供对象存储

### 安装

1. 安装

   ```sh
   # 分别使用iaas-install-swift-controller.sh和iaas-install-swift-compute.sh脚本安装Swift服务并创建test容器
   iaas-install-swift-controller.sh
   iaas-install-swift-compute.sh
   ```



## 安装Cinder块存储服务(2)

### 简介

1. 提供扩展硬盘给nova

### 安装

1. 安装

   ```sh
   # 分别使用iaas-install-cinder-controller.sh、iaas-install-cinder-compute.sh脚本安装Cinder服务
   iaas-install-cinder-controller.sh
   iaas-install-cinder-compute.sh
   ```

2. 创建名字(controller)

   ```
   # 使用cinder命令创建一个名字叫blockvolume，大小为2G的云硬盘
   source /etc/keystone/admin-openrc.sh
   cinder type-create blockvolume
   cinder create 2 --name blockvolume --volume-type blockvolume
   cinder list
   
   
   
   # 对块存储进行扩容操作， 即在计算节点再分出一个 5G 的分区，加入到 cinder 块存储的后端存储中去
   #创建物理卷
   pvcreate /dev/vdb4
   
   #扩展cinder-volume卷组
   vgextend cinder-volumes /dev/vdb4
   
   #验证
   [root@compute ~]# vgdisplay
   
   ```



## 安装DNS服务(controller)

### 简介

1. Designate 是一个开源 DNS 即服务实施，是用于运行云的 OpenStack 服务生态系统的一部分。
   Designate 是 OpenStack 的多租户 DNSaaS 服务。它提供了一个带有集成 Keystone 身份验证的 REST API。它可以配置为根据 Nova 和 Neutron 操作自动生成记录。Designate 支持多种 DNS 服务器，包括 Bind9 和 PowerDNS 4

### 安装

1. 执行脚本

   ```sh
   iaas-install-designate.sh
   ```

   

## 安装Heat编配服务(controller)

### 简介

1. heat支持云平台资源自动部署，集群服务

### 安装

1. 安装

   ```sh
   # 使用iaas-install-heat.sh脚本安装Heat服务
   iaas-install-heat.sh
   
   ```
   



## 安装cloudkitty计费服务(controller)

### 简介

1. 当前Cloudkitty的整体架构，包括计费服务的对象获取（Tenant Fetcher）、计费数据源的收集（Collector）、计费引擎（Rating）的实现，计费费用数据的存储（Storage）

### 安装

1. 安装

   ```sh
   iaas-install-cloudkitty.sh
   ```



## 安装Blazar资源预订服务(controller)

### 简介

1. Blazar 是 OpenStack 的资源预留项目，通过 Blazar 用户可以让 OpenStack 在 “租约（leased）” 期内预留出特定的资源以供使用。

   **预留的资源类型**：

   - **虚拟预留资源**：Nova Instances、Cinder Volumes、Neutron Networks
   - **物理预留资源**：Compute Host（full hosts with specific characteristics of RAM, CPU, etc）

   **应用场景**:

   - 为尖峰负载准备资源
   - 将租约作为计量结算单元
   - 优化能源消耗
   - 申请专用资源

### 安装

1. 安装

   ```
   iaas-install-blazar.sh
   ```



## 安装Ceilometer监控服务(2)

### 简介

1. [Celiometer](https://docs.openstack.org/ceilometer/pike/admin/index.html)是OpenStack的计量与监控组件，官方的正式名称为OpenStack Telemetry，用来获取和保存计量与监控的各种测量值，并根据测量值进行报警。同时这些保存下来的测量值也可以被第三方系统获取，用来做更进一步的分析、处理或展示。

     计量与监控是公有云运营的一个重要环节，计量是为了获取系统中用户对各种资源的使用情况，监控是为了确保资源处于健康的状态

### 安装

1. 安装

   ```
   iaas-install-ceilometer-controller.sh
   iaas-install-ceilometer-compute.sh
   ```



## 安装Aodh告警服务(controller)

### 简介

1. [Openstack](https://so.csdn.net/so/search?q=Openstack&spm=1001.2101.3001.7020)告警服务Aodh负责当收集的数据度量或事件超过所设定的阈值时，会出发报警。从Liberty 版本后从Ceilometer 中拆分出来，独立为单独的项目，Aodh告警可以出发多种形式的动作，目前已经实现的动作有HTTP回调，日志记录和 通过 Zaqar的API发送通知消息

### 安装

1. 安装

   ```
   iaas-install-aodh.sh
   ```

   

# 环境运维

## 镜像管理(controller)

### 查看镜像

```
 glance image-list


# 查看镜像详细信息
glance image-show  ID
```



### 上传镜像

```
# glance image-create 
参数：
 --disk-format：镜像格式
 --container-format： 镜像在其他项目中可见性
 --progress： 显示上传镜像的进度
 --file：选择本地镜像文件
 --name：上传后镜像的名称



source /etc/keystone/admin-openrc.sh
 
 glance image-create --name cirros --disk-format qcow2 --container-format bare < cirros-0.3.4-x86_64-disk.img 
 
 glance image-create --name "CentOS7.5" --disk-format qcow2 --container-format bare --progress < CentOS_7.5_x86_64_XD.qcow2
 
 
 # 创建最小启动10g硬盘、最小启动需要内存1g的cirros
glance image-create --name cirros --disk-format qcow2 --min-disk 10 --min-ram 1024 --container-format bare < cirros-0.3.4-x86_64-disk.img 
```



### 修改镜像

```
glance image-update
选项：
 --min-disk：镜像启动最小硬盘大小
 --name：镜像名称
 --disk-format：镜像格式
 --min-ram：镜像启动最小内存大小
 --container-format：镜像在项目中可见性

# 改变镜像启动硬盘最低要求值（min-disk）1G
glance image-update --min-disk=1 32a2513c-e5ba-438b-a5ee-63c35c03b284
```



### 删除镜像

```
glance image-delete ID


glance image-delete 32a2513c-e5ba-438b-a5ee-63c35c03b284
```



## 网络管理(controller)

### 创建网络（命令）

```
# controller

source /etc/keystone/admin-openrc.sh
openstack network create net
openstack subnet create --network net --subnet-range 10.0.0.0/24 --gateway 10.0.0.1 subnet

openstack subnet create --network net --subnet-range 网络地址 --gateway 网关地址 子网名
```

### 创建网络（图形化）

```
# 创建外网
名称：net-wai
项目：admin
供应商：VLAN
物理网络：provider
段ID:222
四个全选
子网名：in-wai
网络地址：192.168.222.0/24
网关：192.168.222.1
启动DHCP
地址池：192.168.222.3,192.168.222.200



# 创建内网
名称：net-nei
项目：admin
供应商：Flat
物理网络：provider
四个全选
子网名：in-nei
网络地址：192.168.120.0/24
网关：192.168.120.1
关闭DHCP
```



## 云主机管理(controller)

### 创建云主机类型（图像）

```
管理员--》云主机类型--》创建云主机类型

名称：controller
vcpu：2
内存：2048
根磁盘：30
临时磁盘：0
swap：0
因子：默认

注意：创建的类型必须满足镜像最小需求性能
```

### 创建云主机类型（命令）

```
# 创建名为“m1.flavor”、 ID 为 1234、内存为1024MB、硬盘为20GB、vcpu数量为 1的云主机类型。
openstack flavor create --id 1 --ram 1024 --disk 10 --vcpu 1 m1.flavor

openstack flavor create --id id名 --ram 内存 --disk 硬盘容量 --vcpu cpu核数 类型名
```

### 查看云主机类型

```
openstack flavor list 

# 查看centos的详细信息
openstack flavor show centos
```

### 创建云主机

```
# 使用cirros镜像，flavor为2核vCPU、1G内存、10G硬盘，使用network-vlan网络。云主机名为“cirros-test
openstack server create --image cirros-0.4.5 --flavor 1 --netwrok network-clan cirros-test
```

### 查看云主机

```
 openstack server list

# 查看具体信息 
openstack server show cirros-test
```

### 操作云主机

```
openstack server start/stop/reboot cirros-test
```

### 调整云主机

```
# 使用命令调整云主机“cirros-test”类型为centos1，使用–wait参数，在命令执行后，调整云主机需要一定时间，添加–wait参数后会在确认时回馈“complete
openstack server resize --flavor centos1 --wait cirros-test	
```



## 访问安全组

```
# 查看访问安全组
openstack security group list

# 查看安全组default中的安全规则
openstack security group rule list default

# 查看规则的详细信息
openstack security group rule show ID


# 创建安全组
openstack security group create test

# 删除安全组
openstack security group delete test

# 添加安全策略
# 在default中添加策略，从入口方向放行所有ICMP规则
openstack security group rule create --protocol icmp --ingress default
# 在“defualt”安全组中添加一条策略，从入口方向放行所有TCP规则
openstack security group rule create --protocol tcp --ingress  default
# 在“defualt”安全组中添加一条策略，从入口方向放行所有UDP规则
 openstack security group rule create --protocol udp --ingress  default

```



## 块存储管理

```
# 查看服务状态
openstack volume service list

# 查看块存储信息
openstack volume list
# 查看volume1的详细信息
openstack volume show volume1

# 创建块存储
openstack volume create 
# 通过命令创建块存储，大小为2G，名称为“volume1”
openstack volume create --size 2 volume1

# 挂载云硬盘
openstack server add volume
# 使用命令将创建的“volume”块存储添加至云主机“cirros-test”上
openstack server add volume cirros-test volume1

# 调整块存储
openstack volume set
# 通过命令将 “volume1” 卷大小从2G扩容至3G
openstack volume set --size 3 volume1
```



## 对象存储管理

```
# 查看服务状态
swift stat

# 查看容器
openstack container list

# 查看容器详细信息
openstack container show  swift-test1

# 创建容器
openstack container create swift-test1

# 删除容器
openstack container delete
# 删除swift-test1容器（有对象则无法删除）
openstack container delete swift-test1
# 将容器内部对象一起删除
openstack container delete --recursive swift-test1
# 查看对象
openstack object list
# 查看容器swift-test1中所有对象的信息
openstack object list swift-test1

# 查看容器中对象的详细信息
openstack object show swift-test1 test/anaconda-ks.cfg

# 创建对象
# 在使用命令创建对象前，需要将上传后的目录结构在本地创建。在本地创建名为“test”的目录“/root/test”，将/root/anaconda-ks.cfg文件复制至“/root/test”目录中
openstack object create
# 使用命令创建“test/anaconda-ks.cfg
openstack object create swift-test test/anaconda-ks.cfg

# 下载对象
openstack object save
# 使用命令将“swift-test1”容器中“test/anaconda-ks.cfg”对象下载至本地/opt/目录下(会下载到当前目录)
openstack object save swift-test1 test/anaconda-ks.cfg 

# 删除对象
openstack object delete
使用删除对象命令将“swift-test1”容器内“test/anaconda-ks.cfg”删除
openstack object delete swift-test1 test/anaconda-ks.cfg


# 分片存储

# 上传镜像至容器test并进行分片(10m)
swift upload test -S 10000000 cirros-0.3.4-x86_64-disk.img

# 查看cirros镜像的存储路径
swift stat test cirros-0.3.4-x86_64-disk.img

# 查看存储路径中数据片
swift list test_segments

#-S 上载不大于<size>（字节）的段中的文件，然后创建一个“清单”文件，该文件将下载所有段，就像它是原始文件一样。
swift upload examcontainer -S 10485760 cirros-0.3.4-x86_64-disk.img
swift stat examcontainer cirros-0.3.4-x8664-disk.img
swift list  examcontainer_segments



swift upload test -S 10485760 cirros
swift stat test cirros
swift list test_segments

```







## manila共享服务

OpenStack共享文件系统服务（manila）提供对共享或分布式文件系统的协调访问。供应和使用共享的方法由共享文件系统驱动程序或多后端配置情况下的驱动程序确定。有多种驱动程序也支持NFS、CIFS、HDFS、GlusterFS、CEPHFS、MAPRFS和其他协议。

共享文件系统API和调度程序服务通常在控制器节点上运行。根据使用的驱动程序，共享服务可以在控制器、计算节点或存储节点上运行。

------

#### Manila服务安装

（1）编写环境变量文件

使用命令编辑/etc/openstack/openrc.sh配置文件，添加Manila环境变量参数，SHARE_DISK为存储分区名称。

```shell
##--------------------Manila Config----------------##
##Password for Mysql Manila user. exmaple:000000
MANILA_DBPASS=000000

##Password for Keystore Manila user. exmaple:000000
MANILA_PASS=000000

#The NODE Object Disk for Manila. example:md126p5.
SHARE_DISK=vdb3
```

（2）控制节点安装服务

使用CRT等远程连接工具，连接至172.30.17.14控制节点，执行iaas-install-manila-controller.sh安装脚本。

```shell
[root@controller ~]# iaas-install-manila-controller.sh
```

（3）计算节点安装服务

使用CRT等远程连接工具，连接至172.30.17.5计算节点，执行iaas-install-manila-compute.sh安装脚本。

```shell
[root@compute ~]# iaas-install-manila-compute.sh
```

等待脚本执行完毕，即服务安装完毕。

------

####  使用共享服务

（1）创建文件共享类型

使用Manila命令创建default_share_type共享类型，命令如下：

```shell
[root@controller ~]# source /etc/keystone/admin-openrc.sh
# 创建不适用驱动程序支持的default_share_type共享类型
[root@controller ~]# manila type-create default_share_type False
+----------------------+--------------------------------------+
| Property             | Value                                |
+----------------------+--------------------------------------+
| required_extra_specs | driver_handles_share_servers : False |
| Name                 | default_share_type                   |
| Visibility           | public                               |
| is_default           | YES                                  |
| ID                   | 467f8b2b-d1c4-4c79-ae6d-cdbc6ad6f0f8 |
| optional_extra_specs |                                      |
| Description          | None                                 |
+----------------------+--------------------------------------+
```

使用Manila命令查询类型列表信息。命令如下：

```shell
[root@controller ~]# manila type-list
+---------------+-----------+----------+----------+-----------------------+------------+-----------+
| ID            | Name      |visibility|is_default| required_extra_specs  | optional_  |Description|
                                                                            extra_specs
+---------------+-----------+----------+----------+-----------------------+------------+-----------+
| 467f8b2b-d1c4 | default_  | public   | YES      | driver_handles_share_ |            | None      |
  -4c79-ae6d      share_type                         servers : False
  -cdbc6ad6f0f8                               
+---------------+-----------+----------+----------+-----------------------+------------+-----------+
```

（2）创建共享文件目录

使用manila命令创建目录大小为2G的共享目录share01，命令代码如下所示：

```shell
[root@controller ~]# manila create NFS 2 --name share01
+---------------------------------------+--------------------------------------+
| Property                              | Value                                |
+---------------------------------------+--------------------------------------+
| status                                | creating                             |
| share_type_name                       | default_share_type                   |
| description                           | None                                 |
| availability_zone                     | None                                 |
| share_network_id                      | None                                 |
| share_server_id                       | None                                 |
| share_group_id                        | None                                 |
| host                                  |                                      |
| revert_to_snapshot_support            | False                                |
| access_rules_status                   | active                               |
| snapshot_id                           | None                                 |
| create_share_from_snapshot_support    | False                                |
| is_public                             | False                                |
| task_state                            | None                                 |
| snapshot_support                      | False                                |
| id                                    | 6f6bd436-1a64-40bc-acf7-2db3cd0ff892 |
| size                                  | 2                                    |
| source_share_group_snapshot_member_id | None                                 |
| user_id                               | 21323bfecbc44df483ac0120154a43bf     |
| name                                  | share01                              |
| share_type                            | 467f8b2b-d1c4-4c79-ae6d-cdbc6ad6f0f8 |
| has_replicas                          | False                                |
| replication_type                      | None                                 |
| created_at                            | 2022-02-16T09:04:25.000000           |
| share_proto                           | NFS                                  |
| mount_snapshot_support                | False                                |
| project_id                            | 210b13df4d2e4ca1aaaad8712ebd5290     |
| metadata                              | {}                                   |
+---------------------------------------+--------------------------------------+
```

使用manila命令查询所创建的共享目录列表信息，命令代码如下所示：

```shell
[root@controller ~]# manila list
+--------------+-------+----+------+---------+---------+--------------+----------------+-------------+
| ID           | Name  |Size|Share | Status  |Is Public| Share Type   | Host           |Availability |
                             Proto                       Name                           Zone
+--------------+-------+----+------+---------+---------+--------------+----------------+-------------+
|6f6bd436-1a64 |share01| 2  | NFS  |available| False   |default_share |compute@lvm#lvm |nova         |
 -40bc-acf7                                             _type          -single-pool
 -2db3cd0ff892
+--------------+-------+----+------+---------+---------+--------------+----------------+-------------+
```

（3）挂载共享目录

使用manila命令开放share01目录对OpenStack管理网段使用权限，命令代码如下所示：

```shell
[root@controller ~]# manila access-allow share01 ip 172.30.17.0/24 --access-level rw
+--------------+--------------------------------------+
| Property     | Value                                |
+--------------+--------------------------------------+
| access_key   | None                                 |
| share_id     | 6f6bd436-1a64-40bc-acf7-2db3cd0ff892 |
| created_at   | 2022-02-16T09:14:52.000000           |
| updated_at   | None                                 |
| access_type  | ip                                   |
| access_to    | 172.30.17.0/24                       |
| access_level | rw                                   |
| state        | queued_to_apply                      |
| id           | 0dceae9c-c1a9-4df9-bea9-2ba809374b84 |
| metadata     | {}                                   |
+--------------+--------------------------------------+
```

查看share01目录共享目录权限及开放网段，命令如下所示：

```shell
[root@controller ~]# manila access-list share01
+--------------+-----------+--------------+------------+------+----------+----------------+----------+
| id           |access_type| access_to    |access_level|state |access_key| created_at     |updated_at|
+------------- +-----------+--------------+------------+------+----------+----------------+----------+
| 0dceae9c-c1a9| ip        |172.30.17.0/24| rw         |active| None     | 2022-02-16T09: |None      |
  -4df9-bea9                                                               14:52.000000
  -2ba809374b84
+--------------+-----------+--------------+------------+------+----------+----------------+----------+
```

查看share01共享文件目录的访问路径，命令代码如下所示：

```shell
[root@controller ~]#  manila show share01 | grep path | cut -d'|' -f3
 path = 172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724
```

在OpenStack控制节点将share01共享目录挂载至/mnt目录下，命令如下所示：

```shell
[root@controller ~]# mount -t nfs 172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724 /mnt/
```

在控制节点查询挂载信息，可以看到share01共享路径挂载至/mnt目录下。命令如下所示：

```shell
[root@controller ~]# df -hT
文件系统                                                                   类型   容量  已用 可用 已用% 挂载点
devtmpfs                                                                devtmpfs  5.8G  0    5.8G    0% /dev
tmpfs                                                                      tmpfs  5.8G  68K  5.8G    1% /dev/shm
tmpfs                                                                      tmpfs  5.8G  592M 5.3G   10% /run
tmpfs                                                                      tmpfs  5.8G  0    5.8G    0% /sys/fs/cgroup
/dev/vda1                                                                    xfs  50G   8.1G  42G   17% /
tmpfs                                                                      tmpfs  1.2G  0    1.2G    0% /run/user/0
172.30.17.5:/var/lib/manila/mnt/share-c3f5a9fc-a8e7-40a6-a43b-56cfd1738724 nfs4   2.0G  6.0M 1.8G    1% /mnt
```

至此，Manila共享文件服务安装完成。在生产环境中，Manila共享文件服务所能提供的存储空间足够满足用户使用，这里因是实验环境，只创建一个2G共享空间用于演示。Manila给用户和服务提供一个共享文件存储空间，与Cinder和Swift服务并不一样。







## cloudkitty计费服务

当前版本cloudkitty可以完成虚拟机实例（compute），云硬盘（volume），镜像（image），网络进出流量（network.bw.in,network.bw.out），浮动IP（network.floating）的计费。得益于Cloudkitty的巧妙而优秀的设计，软件插件化思想更是体现的淋漓尽致，使得添加新的计费源异常容易，版本升级十分方便。同样也能方便将cloudkitty用于cloudstack，VMWare等环境中。

Cloudkitty主要依赖于遥测相关的项目，包括ceilometer和gnocchi，甚至是将要使用panko；计费策略和hashmap计费模型是其核心；模块插件化是其设计灵魂；

------

####  Cloudkitty服务安装

（1）编写环境变量文件

使用命令编辑/etc/openstack/openrc.sh配置文件，添加Cloudkitty环境变量参数。

```shell
##--------------------Cloudkitty Config----------------##
##Password for Mysql Cloudkitty user. exmaple:000000
CLOUDKITTY_DBPASS=000000

##Password for Keystore Cloudkitty user. exmaple:000000
CLOUDKITTY_PASS=000000
```

（2）控制节点安装服务

使用CRT等远程连接工具，连接至172.30.17.14控制节点，执行iaas-install- cloudkitty.sh安装脚本。

```shell
[root@controller ~]# iaas-install-cloudkitty.sh
```

#### 使用Cloudkitty服务

（1）类型规格费用

创建云主机服务instance_test，通过命令创建service服务。命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap service create instance_test
+---------------+--------------------------------------+
| Name          | Service ID                           |
+---------------+--------------------------------------+
| instance_test | cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 |
+---------------+--------------------------------------+
[root@controller ~]#
```

并对其创建名为flavor_name的fields，使用命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap field create cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 flavor_name
+-------------+--------------------------------------+--------------------------------------+
| Name        | Field ID                             | Service ID                           |
+-------------+--------------------------------------+--------------------------------------+
| flavor_name | b2f0d485-df20-4f2e-bd44-d3696971cb8f | cf8029bf-dc35-4e40-b8fd-5af4a4d25a30 |
+-------------+--------------------------------------+--------------------------------------+
```

并设置规格为m1.small的云主机单价为1元，使用命令如下所示：

```shell
[root@controller ~]# openstack rating hashmap mapping create  --field-id b2f0d485-df20-4f2e-bd44-d3696971cb8f  -t flat --value  m1.small 1
+--------------------+--------+----------+----+--------------------+----------+--------+----------+
| Mapping ID         |Value   |Cost      |Type| Field ID           |Service ID|Group ID|Project ID|
+--------------------+--------+----------+----+--------------------+----------+--------+----------+
| c1b7d4db-c1d2-4488 |m1.small|1.00000000|flat| b2f0d485-df20-4f2e | None     | None   | None     |
  -ac46-1a8eb70d76e4                            -bd44-d3696971cb8f
+--------------------+--------+----------+----+--------------------+----------+--------+----------+
```

（2）镜像服务费用

创建镜像收费服务image_size_test，命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap service create image_size_test
+-----------------+--------------------------------------+
| Name            | Service ID                           |
+-----------------+--------------------------------------+
| image_size_test | 80a098cf-d793-47cf-b63e-df6cbd56e88d |
+-----------------+--------------------------------------+
```

并为该服务单价设置为0.8元，命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap mapping create -s  80a098cf-d793-47cf-b63e-df6cbd56e88d   -t flat 0.8 
+--------------------+-------+------------+------+----------+--------------------+--------+----------+
| Mapping ID         | Value | Cost       | Type | Field ID | Service ID         |Group ID|Project ID|
+--------------------+-------+------------+------+----------+--------------------+--------+----------+
| 64952e70-6e37-4c8a | None  | 0.80000000 | flat | None     | 80a098cf-d793-47cf | None   | None     |
  -9d3a-b4c70de1fb87                                          -b63e-df6cbd56e88d
+--------------------+-------+------------+------+----------+--------------------+--------+----------+
```

（3）创建优惠服务

创建名为dis_tests的服务，命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap service create dis_tests
+-----------+--------------------------------------+
| Name      | Service ID                           |
+-----------+--------------------------------------+
| dis_tests | b6190077-80ff-4e1f-9938-ddaff11c3506 |
+-----------+--------------------------------------+
```

为dis_tests服务设置单价为0.8元，命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap mapping create -s  b6190077-80ff-4e1f-9938-ddaff11c3506  -t flat 0.8
+--------------------+-------+------------+----+--------+--------------------+----------+------------+
| Mapping ID         | Value | Cost       |Type|Field ID| Service ID         | Group ID | Project ID |
+--------------------+-------+------------+----+--------+--------------------+----------+------------+
| f5a51b26-82f8-45a4 | None  | 0.80000000 |flat| None   | b6190077-80ff-4e1f | None     | None       |
  -8784-f7f131c2e4cd                                      -9938-ddaff11c3506
+--------------------+-------+------------+----+--------+--------------------+----------+------------+
```

并设置dis_tests服务使用量超过10000G时提供8折优惠，命令代码如下所示：

```shell
[root@controller ~]# openstack rating hashmap threshold create -s b6190077-80ff-4e1f-9938-ddaff11c3506 -t rate 10000 0.8

+-------------------+--------------+----------+----+--------+--------------------+--------+----------+
| Threshold ID      | Level        |Cost      |Type|Field ID| Service ID         |Group ID|Project ID|
+-------------------+--------------+----------+----+--------+--------------------+--------+----------+
|11be2ecb-b7fa-42cd |10000.00000000|0.80000000|rate| None   | b6190077-80ff-4e1f | None   | None     |
 -a8d1-a0b679181912                                           -9938-ddaff11c3506
+-------------------+--------------+----------+----+--------+--------------------+--------+----------+
```







## Barbican密钥管理器服务

Key Manager 服务 (barbican) 提供机密数据的安全存储、配置和管理。这包括密钥材料，例如对称密钥、非对称密钥、证书和原始二进制数据。

Barbican 是 OpenStack 的key管理组件，定位在提供 REST API 来安全存储、提供和管理“秘密”。

Barbican最常用的功能是作为OpenStack环境中的密钥生成器，为各种加解密操作提供支持;同时，Barbican还可以保存OpenStack环境中的用户机密数据。

------

#### Barbican服务安装

（1）编写环境变量文件

使用命令编辑/etc/openstack/openrc.sh配置文件，添加Barbican环境变量参数。

```shell
##--------------------Barbican Config----------------##
##Password for Mysql Barbican user. exmaple:000000
BARBICAN_DBPASS=000000

##Password for Keystore Barbican user. exmaple:000000
BARBICAN_PASS=000000
```

（2）控制节点安装服务

使用CRT等远程连接工具，连接至172.30.17.14控制节点，执行iaas-install-barbican.sh安装脚本

```shell
[root@controller ~]# iaas-install-barbican.sh
```

------

#### 使用Barbican服务

使用openstack命令创建一个名为secret01的secret，命令代码如下所示：

```shell
[root@controller ~]# openstack secret store --name secret01 --payload secretkey
+---------------+------------------------------------------------------------------------+
| Field         | Value                                                                  |
+---------------+------------------------------------------------------------------------+
| Secret href   | http://controller:9311/v1/secrets/08e96ae0-b727-4583-afd9-8ab9de507d4a |
| Name          | secret01                                                               |
| Created       | None                                                                   |
| Status        | None                                                                   |
| Content types | None                                                                   |
| Algorithm     | aes                                                                    |
| Bit length    | 256                                                                    |
| Secret type   | opaque                                                                 |
| Mode          | cbc                                                                    |
| Expiration    | None                                                                   |
+---------------+------------------------------------------------------------------------+
```

查询secret列表信息，命令代码如下所示：

```shell
[root@controller ~]# openstack secret list
+------------------+--------+---------+------+--------------+---------+------+-------+----+----------+
| Secret href      | Name   | Created |Status| Content types|Algorithm|Bit   |Secret |Mode|Expiration|
                                                                       length type   
+------------------+--------+---------+------+--------------+---------+------+-------+----+----------+
|http://controller:|secret01|2022-02- |ACTIVE|{u'default':  | aes     |  256 | opaque|cbc | None     |
 9311/v1/secrets/0           17T09:03          u'text/plain'}
 8e96ae0-b727-4583           :20+00:00
 -afd9-8ab9de507d4a
+------------------+--------+---------+------+--------------+---------+------+-------+----+----------+
```

使用命令获取secret01密钥的元数据，命令代码如下所示：

```shell
[root@controller ~]# openstack secret get http://controller:9311/v1/secrets/08e96ae0-b727-4583-afd9-8ab9de507d4a
+---------------+------------------------------------------------------------------------+
| Field         | Value                                                                  |
+---------------+------------------------------------------------------------------------+
| Secret href   | http://controller:9311/v1/secrets/08e96ae0-b727-4583-afd9-8ab9de507d4a |
| Name          | secret01                                                               |
| Created       | 2022-02-17T09:03:20+00:00                                              |
| Status        | ACTIVE                                                                 |
| Content types | {u'default': u'text/plain'}                                            |
| Algorithm     | aes                                                                    |
| Bit length    | 256                                                                    |
| Secret type   | opaque                                                                 |
| Mode          | cbc                                                                    |
| Expiration    | None                                                                   |
+---------------+------------------------------------------------------------------------+
```

通过命令获取secret01密钥的数据，命令代码如下所示：

```shell
[root@controller ~]# openstack secret get http://controller:9311/v1/secrets/08e96ae0-b727-4583-afd9-8ab9de507d4a --payload
+---------+-----------+
| Field   | Value     |
+---------+-----------+
| Payload | secretkey |
+---------+-----------+
```

使用openstack命令生成并存储密钥，命令代码如下所示：

```shell
# 生成一个新的 256 位密钥并将其存储在 barbican 中
[root@controller ~]# openstack secret order create --name secret02 --algorithm aes --bit-length 256 --mode cbc --payload-content-type application/octet-stream key
+----------------+-----------------------------------------------------------------------+
| Field          | Value                                                                 |
+----------------+-----------------------------------------------------------------------+
| Order href     | http://controller:9311/v1/orders/421a8256-79d2-4e53-80ee-c493d4a81317 |
| Type           | Key                                                                   |
| Container href | N/A                                                                   |
| Secret href    | None                                                                  |
| Created        | None                                                                  |
| Status         | None                                                                  |
| Error code     | None                                                                  |
| Error message  | None                                                                  |
+----------------+-----------------------------------------------------------------------+
```

通过命令显示生成的密钥列表，命令代码如下所示：

```shell
[root@controller ~]# openstack secret order list
+-------------------------+----+---------+------------------------+----------+------+------+-------+
| Order href              |Type|Container| Secret href            | Created  |Status|Error |Error  |
                                href                                                 code   message
+-------------------------+----+---------+------------------------+----------+------+------+-------+
|http://controller:9311/v1| Key| N/A     |http://controller:9311/v|2022-02-09|ACTIVE| None | None  |
 /orders/798791cb-297c-4a                 1/secrets/f77fcc45-750e  T08:36:30+
 3d-a8c7-30f6a4dfdd3a                     -4ed2-903d-9af076c26586  00:00
|http://controller:9311/v1| Key| N/A     |http://controller:9311/v|2022-02-17|ACTIVE| None | None |
 /orders/5675a750-bf8f-46                 1/secrets/81b7a19b-2975  T09:13:29+
 68-862c-cb8eedcbb42a                     -4ff0-99a5-ccd5581d1cfa  00:00
+-------------------------+----+---------+------------------------+----------+------+------+-------+
```

使用命令显示生成的密钥，命令代码如下所示：

```shell
[root@controller ~]# openstack secret order get http://controller:9311/v1/orders/5675a750-bf8f-4668-862c-cb8eedcbb42a
+----------------+------------------------------------------------------------------------+
| Field          | Value                                                                  |
+----------------+------------------------------------------------------------------------+
| Order href     | http://controller:9311/v1/orders/5675a750-bf8f-4668-862c-cb8eedcbb42a  |
| Type           | Key                                                                    |
| Container href | N/A                                                                    |
| Secret href    | http://controller:9311/v1/secrets/81b7a19b-2975-4ff0-99a5-ccd5581d1cfa |
| Created        | 2022-02-17T09:30:14+00:00                                              |
| Status         | ACTIVE                                                                 |
| Error code     | None                                                                   |
| Error message  | None                                                                   |
+----------------+------------------------------------------------------------------------+
```

显示生成的密钥的元数据，命令代码如下所示：

```shell
[root@controller ~]# openstack secret get http://controller:9311/v1/secrets/81b7a19b-2975-4ff0-99a5-ccd5581d1cfa 
+---------------+------------------------------------------------------------------------+
| Field         | Value                                                                  |
+---------------+------------------------------------------------------------------------+
| Secret href   | http://controller:9311/v1/secrets/81b7a19b-2975-4ff0-99a5-ccd5581d1cfa |
| Name          | secret01                                                               |
| Created       | 2022-02-17T09:30:14+00:00                                              |
| Status        | ACTIVE                                                                 |
| Content types | {u'default': u'application/octet-stream'}                              |
| Algorithm     | aes                                                                    |
| Bit length    | 256                                                                    |
| Secret type   | symmetric                                                              |
| Mode          | cbc                                                                    |
| Expiration    | None                                                                   |
+---------------+------------------------------------------------------------------------+
```

至此，barbican密钥管理器服务安装完成，上述实验中，只对应熟悉barbican密钥服务所使用的环境操作。









## VPNaaS服务

在Openstack的Havana版本中，Neutron增加的一个功能就是VPNaas，将VPN功能引入到了Neutron中，虽然在Havana版中只支持IPSec协议的VPN，功能还比较弱，但这个关键特性已经有了，后续会不断加强该特性。

VPN可以通过在L2或L3层建立一条逻辑链路，让广域网上多个内网能够相互访问。VPN的实现方式很多，有基于租用专用物理线路实现的，也有基于以太网的虚连接的实现方式。

------

#### VPNaaS服务安装

（1）控制节点安装服务

使用CRT等远程连接工具，连接至172.30.17.14控制节点，执行iaas-install-fwaas-and-vpnaas.sh安装脚本 。

```shell
[root@controller ~]# iaas-install-fwaas-and-vpnaas.sh
```

------

#### VPNaaS服务使用

（1）VPNaaS服务网络拓扑

我们可以在单个集群中的不同租户（admin和demo租户）内，分别创建内部网络net1和net2，并分别给网络net1和net2添加到路由route1和route2中，通过一个共享的外网网络ext-net来完成VPNaas的隧道构建，使处于不同租户网络的实例网络可以互通。

验证网络拓扑如下所示。

```shell
(100.0.1.0/24 – admin租户)
              |
              |  100.0.1.1
           [route1]
              |  100.0.0.11
              |
           [ext-net]
              |-------------------VPNaas服务
           [ext-net]
              |  
              |  100.0.0.22
          [ route2]
              |  10.2.0.1
              |
(100.0.2.0/24 demo租户)
```

（2）创建路由网络

在控制节点/root/目录下编写路由网络创建脚本route-net-build.sh，脚本内容如下所示：

```shell
#/bin/bash

#admin租户创建路由网络
source /etc/keystone/admin-openrc.sh
#创建vxlan外网网络
openstack network create --external --share ext-net
openstack subnet create --subnet-range 100.0.0.0/24 --gateway 100.0.0.1 --network ext-net ext-subnet
#创建vxlan内网网络net1
openstack network create net1
openstack subnet create --subnet-range 100.0.1.0/24 --gateway 100.0.1.1 --network net1 net1
#创建路由route1，网关100.0.0.11，添加内网net1
openstack router create route1
openstack router set --external-gateway ext-net --fixed-ip subnet=ext-subnet,ip-address=100.0.0.11 route1
openstack router add subnet route1 net1


#demo租户创建路由网络
source /etc/keystone/demo-openrc.sh
#创建vxlan内网网络net2
openstack network create net2
openstack subnet create --subnet-range 100.0.2.0/24 --gateway 100.0.2.1 --network net2 net2
#创建路由route2，网关100.0.0.22，添加内网net2
openstack router create route2 
source /etc/keystone/admin-openrc.sh
openstack router add subnet route2 net2
openstack router set --external-gateway ext-net --fixed-ip subnet=ext-subnet,ip-address=100.0.0.22 route2
```

赋予脚本route-net-build.sh执行权限。命令如下。

```shell
[root@controller ~]# chmod +x route-net-build.sh
```

执行脚本route-net-build.sh，完成路由网络的创建。命令如下。

```shell
[root@controller ~]# ./route-net-build.sh
```

（3）构建VPN连接

在admin租户创建vpn连接，–peer-address为demo租户的路由route2网关地址100.0.0.22。命令如下。

```shell
[root@controller ~]# source /etc/keystone/admin-openrc.sh
[root@controller ~]# openstack vpn ike policy create ikepolicy1
[root@controller ~]# openstack vpn ipsec policy create ipsecpolicy1
[root@controller ~]# openstack vpn service create --router route1 --subnet net1 vpn1
[root@controller ~]# openstack vpn ipsec site connection create vpnconnectiona --vpnservice vpn1 --ikepolicy ikepolicy1  --ipsecpolicy ipsecpolicy1 --peer-address 100.0.0.22 --peer-id 100.0.0.22 --peer-cidr 100.0.2.0/24 --psk secret

```

在demo租户创建vpn连接，–peer-address为admin租户的路由route1网关地址100.0.0.11。命令如下。

```shell
[root@controller ~]# source /etc/keystone/demo-openrc.sh
[root@controller ~]# openstack vpn ike policy create ikepolicy2
[root@controller ~]# openstack vpn ipsec policy create ipsecpolicy2
[root@controller ~]# openstack vpn service create --router route2 --subnet net2 vpn2
[root@controller ~]# openstack vpn ipsec site connection create vpnconnectionb --vpnservice vpn2 --ikepolicy ikepolicy2  --ipsecpolicy ipsecpolicy2 --peer-address 100.0.0.11 --peer-id 100.0.0.11 --peer-cidr 100.0.1.0/24 --psk secret
```

（4）验证vpn连接

查看vpn连接状态（如果状态为PENDING_CREATE，请稍微等待几分钟）。命令如下。

```shell
[root@controller ~]# source /etc/keystone/admin-openrc.sh 
[root@controller ~]# openstack vpn ipsec site connection list
+------------------------------------+--------------+------------+------------------------+--------+
| ID                                 |Name          |Peer Address|Authentication Algorithm| Status |
+------------------------------------+--------------+------------+------------------------+--------+
|20db9a99-f7dd-46bb-81fa-f6ce1fdcb996|vpnconnectiona| 100.0.0.22 | psk                    | ACTIVE |
|c74e2cb8-5f0a-4f13-bf60-95915641baed|vpnconnectionb| 100.0.0.11 | psk                    | ACTIVE |
+------------------------------------+--------------+------------+------------------------+--------+
```





## NFS作为Glance存储后端

（1）NFS简介

NFS网络文件系统提供了一种在类UNIX系统上共享文件的方法。目前NFS有3个版本：NFSv2、NFSv3、NFSv4。CentOS7默认使用NFSv4提供服务，优点是提供了有状态的连接，更容易追踪连接状态，增强安全性。NFS监听在TCP 2049端口上。客户端通过挂载的方式将NFS服务器端共享的数据目录挂载到本地目录下。在客户端看来，使用NFS的远端文件就像是在使用本地文件一样，只要具有相应的权限就可以使用各种文件操作命令（如cp、cd、mv和rm等），对共享的文件进行相应的操作。Linux操作系统既可以作为NFS服务器也可以作为NFS客户，这就意味着它可以把文件系统共享给其他系统，也可以挂载从其他系统上共享的文件系统。

为什么需要安装NFS服务？当服务器访问流量过大时，需要多台服务器进行分流，而这多台服务器可以使用NFS服务进行共享。NFS除了可以实现基本的文件系统共享之外，还可以结合远程网络启动，实现无盘工作站（PXE启动系统，所有数据均在服务器的磁盘阵列上）或瘦客户工作站（本地自动系统）。NFS应用场景多为高可用文件共享，多台服务器共享同样的数据，但是它的可扩展性比较差，本身高可用方案不完善。取而代之，数据量比较大的可以采用MFS、TFS、HDFS等分布式文件系统。

（2）NFS组成

两台计算机需要通过网络建立连接时，双方主机就一定需要提供一些基本信息，如IP地址、服务端口号等。当有100台客户端需要访问某台服务器时，服务器就需要记住这些客户端的IP地址以及相应的端口号等信息，而这些信息是需要程序来管理的。在Linux中，这样的信息可以由某个特定服务自己来管理，也可以委托给RPC来帮助自己管理。RPC是远程过程调用协议，RPC协议为远程通信程序管理通信双方所需的基本信息，这祥，NFS服务就可以专注于如何共享数据。至于通信的连接以及连接的基本信息，则全权委托给RPC管理。因此，NFS组件由与NFS相关的内核模块、NFS用户空间工具和RPC相关服务组成。主要由如下2个RPM包提供。

① nfs-utils：包含NFS服务器端守护进程和NFS客户端相关工具。

② rpcbind：提供RPC的端口映射的守护进程及其相关文档、执行文件等。



与NFS服务相关的文件有：守护进程、systemd的服务配置单元、服务器端配置文件、客户端配置文件、服务器端工具、客户端工具、NFS信息文件等。

------

####  NFS服务安装与配置

（1）修改主机名

修改主机名，修改主机名后，使用CRT软件断开重新连接节点，以生效新主机名。

```shell
# 关闭防火墙

# 服务端安装
# yum install nfs-utils rpcbind

# systemctl start rpcbind
# systemctl start nfs
# systemctl enable rpcbind
# systemctl enable nfs-server

# 客户端安装
# yum install -y nfs-utils

[root@localhost ~]# hostnamectl set-hostname nfs-server
[root@localhost ~]# hostname
nfs-server
```

（2）NFS服务安装

此处nfs-server节点使用的基础镜像为CentOS7.9，该基础镜像中已经安装了NFS服务，查看当前安装的NFS服务，命令如下：

```shell
[root@nfs-server ~]# rpm -qa |grep nfs-utils
nfs-utils-1.3.0-0.68.el7.x86_64
[root@nfs-server ~]# rpm -qa |grep rpcbind
rpcbind-0.2.0-49.el7.x86_64
```

可以看到nfs-utils和rpcbind服务已经安装完毕了。

（3）NFS服务配置

创建一个目录作为NFS的共享目录，命令如下：

```shell
[root@nfs-server ~]# mkdir /mnt/test
[root@nfs-server ~]# ll /mnt/
total 0
drwxr-xr-x. 2 root root 6 Feb  9 05:56 test
```

创建完共享目录后，编辑NFS服务的配置文件/etc/exports，在配置文件中加入一行代码，命令如下：

```shell
[root@nfs-server ~]# vi /etc/exports
[root@nfs-server ~]# cat /etc/exports
/mnt/test 192.168.200.0/24(rw,no_root_squash,no_all_squash,sync,anonuid=501,anongid=501)
```

生效配置，命令如下：

```shell
[root@nfs-server ~]# exportfs -r
```

配置文件说明：

● /mnt/test：为共享目录（若没有这个目录，请新建一个）。

● 192.168.200.0/24：可以为一个网段，一个IP，也可以是域名。域名支持通配符，例如，*.qq.com。

● rw：read-write，可读写。

● ro：read-only，只读。

● sync：文件同时写入硬盘和内存。

● async：文件暂存于内存，而不是直接写入内存。

● wdelay：延迟写操作。

● no_root_squash：NFS客户端连接服务端时，如果使用的是root，那么对服务端共享的目录来说，也拥有root权限。显然开启这项是不安全的。

● root_squash：NFS客户端连接服务端时，如果使用的是root，那么对服务端共享的目录来说，拥有匿名用户权限，通常它将使用nobody或nfsnobody身份。

● all_squash：不论NFS客户端连接服务端时使用什么用户，对服务端共享的目录来说，都拥有匿名用户权限。

● anonuid：匿名用户的UID（User Identification，用户身份证明）值，可以在此处自行设定。

● anongid：匿名用户的GID（Group Identification，共享资源系统使用者的群体身份）值。

（4）NFS服务启动

nfs-server端启动NFS服务，命令如下：

```shell
[root@nfs-server ~]# systemctl start rpcbind
[root@nfs-server ~]# systemctl start nfs
```

nfs-server端查看可挂载目录，命令如下：

```shell
[root@nfs-server ~]# showmount -e 192.168.200.31
Export list for 192.168.200.31:
/mnt/test 192.168.200.0/24
```

至此，NFS服务的Server端配置完毕，接下来要让Controller节点作为NFS的Client端，配置Glance服务的后端存储使用NFS服务。

------

####  配置NFS作为Glance后端存储

（1）配置Controller节点作为Client端

使用远程工具连接到Controller节点，查看是否安装了NFS服务的客户端，命令如下：

```shell
[root@controller ~]# rpm -qa |grep nfs-utils
nfs-utils-1.3.0-0.61.el7.x86_64
```

查看到Controller节点已经安装了nfs-utils工具，该节点可以作为NFS的Client端使用。

（2）挂载目录

在挂载目录之前，必须要弄清楚一件事情，就是Glance服务的后端存储在哪里，或者说，使用glance image-create命令上传的镜像会被存放到哪里。镜像会被存放到/var/lib/glance/images目录下，关于这个路径，感兴趣的读者可以自行上传镜像测试。

知道了Glance的存储路径，就可以挂载该目录到NFS服务了，命令如下：

```shell
[root@controller ~]# mount -t nfs 192.168.200.31:/mnt/test /var/lib/glance/images/
```

使用df命令查看挂载情况。

```shell
[root@controller ~]# df -h
Filesystem               Size  Used Avail Use% Mounted on
/dev/vda1                 50G  2.7G   48G   6% /
devtmpfs                 5.8G     0  5.8G   0% /dev
tmpfs                    5.8G  4.0K  5.8G   1% /dev/shm
tmpfs                    5.8G   49M  5.8G   1% /run
tmpfs                    5.8G     0  5.8G   0% /sys/fs/cgroup
/dev/loop0               9.4G   33M  9.4G   1% /swift/node/vda5
tmpfs                    1.2G     0  1.2G   0% /run/user/0
192.168.200.31:/mnt/test   20G  865M   20G   5% /var/lib/glance/images
```

可以发现挂载成功。

（3）修改配置

在做完挂载操作后，此时Glance服务还不能正常使用，若使用glance image-create命令上传镜像的话，会报错，因为此时images目录的用户与用户组不是glance，而是root，需要把images目录的用户与用户组进行修改，命令如下：

```shell
[root@controller ~]# cd /var/lib/glance/
[root@controller glance]# chown glance:glance images/
[root@controller glance]# ll
total 0
drwxr-xr-x. 2 glance glance 6 Feb  9 05:56 images
```

这个时候，Glance服务就可以正常使用了，使用cirros镜像进行测试，将

cirros-0.3.4-x86_64-disk.img上传至Controller节点，并上传，命令如下：

```shell
[root@controller ~]# source /etc/keystone/admin-openrc.sh 
[root@controller ~]# glance image-create --name cirros --disk-format qcow2 --container-format bare --progress ] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     |
| container_format | bare                                 |
| created_at       | 2022-02-09T06:22:39Z                 |
| disk_format      | qcow2                                |
| id               | a59c327b-0c0d-44e7-a0f2-f9d53761c2b4 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros                               |
| owner            | 55b50cbb4dd4459b873cb15a8b03db43     |
| protected        | False                                |
| size             | 13287936                             |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2022-02-09T06:22:40Z                 |
| virtual_size     | None                                 |
| visibility       | shared                               |
+------------------+--------------------------------------+
```

可以看到上传镜像成功。查看images目录下的文件，命令如下：

```shell
[root@controller ~]# ll /var/lib/glance/images/
total 12980
-rw-r-----. 1 glance glance 13287936 Feb  9 06:22 a59c327b-0c0d-44e7-a0f2-f9d53761c2b4
```

然后回到nfs-server节点，查看/mnt/test下的文件，命令如下：

```shell
[root@nfs-server ~]# ll /mnt/test/
total 12980
-rw-r-----. 1 161 161 13287936 Feb  9 06:22 a59c327b-0c0d-44e7-a0f2-f9d53761c2b4
```

文件的ID相同，验证NFS作为Glance镜像服务的后端存储成功。







## RabbitMQ服务

RabbitMQ是Advanced Message Queuing Protocol （AMQP，高级消息队列协议）开放标准的实现，它支持符合标准的客户端请求程序与符合标准的消息中间件代理进行通信。AMQP的模型架构如图2-1所示：

![null](私有云部署.assets/wKggBmIgapOACnubAAFZpyw-waA709.png)

图2-1 AMQP模型

AMQP中的核心概念：

（1）Broker：消息中间件的服务节点，对于RabbitMQ来说，一个RabbitMQ Broker可以简单地看作一个RabbitMQ服务节点，或者RabbitMQ服务实例；

（2）Virtual Host：虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个vhost本质上就是一个mini版的RabbitMQ服务器，拥有自己的队列、交换器、绑定和权限机制；

（3）Producer：生产者，消息投递方，生产者创建消息，然后发布到RabbitMQ中；

（4）Consumer：消费者，就是接收消息的一方，消费者连接到RabbitMQ服务器，并订阅到队列上；

（5）Queue：队列，RabbitMQ的内部对象，用于存储消息，RabbitMQ的生产者生产消息并最终投递到队列中，消费者可以从队列中获取消息并消费；

（6）Exchange：交换器，生产者将消息发送到Exchange，由交换器将消息路由到一个或者多个队列中；

（7）RoutingKey：路由键，生产者将消息发给交换器的时候，一般会指定一个RoutingKey，用来指定这个消息的路由规则，生产者可以在发送消息给交换器时，通过指定RoutingKey来决定消息流向哪里。

------

#### RabbitMQ在OpenStack中的运用

要了解RabbitMQ在OpenStack中的作用，首先以创建虚拟机为例分析一下消息流程，创建虚拟机的流程如图2-2所示：

![null](私有云部署.assets/wKggBmIgaqqABf16AAD4rbtFwq0426.png)

图2-2 创建虚拟机流程图

从上图能够看出，以nova-api和nova-conductor之间的通信为例，nova-conductor服务在启动时会注册一个RPC server等待处理请求，nova-api发送创建虚拟机的rpc请求时会先创建一个topic publisher用于topic发布，method为build_instance，然后publisher将消息发送给exchange，exchange再根据routingkey转发给绑定的queue中，最后由topic consumer接收并调用nova-conductor manager中的build_instance方法处理，对于nova-conductor和nova-scheduler之间的通信，多了一步把目标主机作为返回结果信息返回到reply_xx队列中，然后由nova-conductor接收以后向nova-compute发起rpc.cast的创建请求。

OpenStack各个组件内部的各个服务进程之间则是通过基于AMPQ的RPC方式进行通信，实现RPC通信需借助Rabbitmq消息队列，RPC方式又分为两种，rpc.cast和rpc.call，rpc.call为request/response方式，多用于同步场景；而使用rpc.cast方式发出请求后则无需一直等待响应，但之后需要定期查询执行结果，一般用于异步场景，OpenStack将其使用的通信方式都封装在公有库oslo_messaging中。

------

#### RabbitMQ的性能瓶颈

RabbitMQ每增加一个连接，erlang都会给这个连接分配三个erlang进程，每个进程都会分配一定大小内存空间，所以随着连接数的增长，内存和erlang进程数呈现有规律的增长，所以RabbitMQ连接数的无限增大会压垮mq服务，导致RabbitMQ服务崩溃。

客户端与RabbitMQ建立的是长连接，而不是建立短连接，因为如果频繁的建立、销毁connection，会增加额外的时间开销，当业务量比较大时，就会对系统性能产生比较大的影响。OpenStack组件与RabbitMQ的连接使用到了第三方库oslo_message中的connection pool的概念，在不超过pool size的前提上，当有并发业务的时候，如果发现pool中已有connection正被使用，那么就会在pool中继续创建新的connection，直到创建的connection数量达到pool的最大值，之后如果再有业务需要，会等待之前创建的connection被重新放入connection pool，然后等待被继续使用。这种情况下，就会出现connection一直增长的现象。

------

####  RabbitMQ的优化

在上面的文章中可以看到，RabbitMQ的连接数是压垮消息队列的一个重要的指标。所以在平时使用OpenStack平台的过程中，如果大量的用户同时创建虚拟机，会导致云平台创建报错，其实就是消息队列服务的崩溃。

在优化方面，我们首先想到，是将RabbitMQ服务默认的连接数量改大，修改方法如下：

（1）系统级别修改

使用CRT等远程工具连接到controller节点，然后修改配置文件，命令如下：

```shell
[root@controller ~]# vi /etc/sysctl.conf
fs.file-max=10240
#在sysctl.conf文件的最下方添加一行fs.file-max=10240
```

修改完毕后保存退出并生效配置，命令如下：

```shell
[root@controller ~]# sysctl -p
fs.file-max = 10240
```

（2）用户级别修改

用户级别修改，编辑/etc/security/limits.conf配置文件，具体命令如下：

```shell
[root@controller ~]# vi /etc/security/limits.conf
openstack  soft     nofile  10240
openstack  hard     nofile  10240
#在配置文件的最后添加两行内容如上
```

修改完之后，保存退出。

 修改RabbitMQ配置

修改RabbitMQ服务的service配置文件rabbitmq-server.service，具体命令如下：

```shell
[root@controller ~]# vi /usr/lib/systemd/system/rabbitmq-server.service
#在[Service]下添加一行参数如下：
LimitNOFILE=10240
```

编辑完之后保存退出，重启RabbitMQ服务，命令如下：

```shell
[root@controller ~]# systemctl daemon-reload
[root@controller ~]# systemctl restart rabbitmq-server
```

重启完毕后，查看RabbitMQ的最大连接数，命令如下：

```shell
[root@controller ~]# rabbitmqctl status
Status of node rabbit@openstack
...忽略输出...
 {file_descriptors,
     [{total_limit,10140},
      {total_used,53},
      {sockets_limit,9124},
      {sockets_used,51}]},
```

可以看到当前的RabbitMQ已被修改





## 开放镜像权限

（1）背景

某OpenStack云平台有两个租户，A租户与B租户，分别属于两个部门，该公司对镜像的管理比较严格，镜像都由管理员进行上传和权限管理。

（2）诉求

该公司有一个镜像，需要共享给A租户使用，对B租户不可见，实现这种方式最简单的方式，是由A租户中的用户自行上传镜像，这样A租户里面的用户可以看见该镜像，而B租户中的用户看不见。但是现在镜像不能由普通用户去上传，只能通过管理员进行操作。

（3）解决方案

通过管理员上传该镜像，并使用相关命令开放给A租户。

------

####  案例实操

（1）创建租户

登录OpenStack平台，创建项目depA和depB，并且在这两个项目下各创一个用户userA和userB的普通用户，创建完之后（创建过程不再赘述），使用命令查看，命令如下：

```shell
[root@controller ~]# source /etc/keystone/admin-openrc.sh 
[root@controller ~]# openstack project list
+----------------------------------+---------+
| ID                               | Name    |
+----------------------------------+---------+
| 0dd87985eb314fed828e6888aed4880d | demo    |
| 525075abb84e4e088dfe3adc4da61e72 | depB    |
| 55b50cbb4dd4459b873cb15a8b03db43 | admin   |
| a184a157399043c2a40abc52df0459a2 | service |
| df58511d2c914690b48e89f1e512ae6b | depA    |
+----------------------------------+---------+
[root@controller ~]# openstack user list
+----------------------------------+-------------------+
| ID                               | Name              |
+----------------------------------+-------------------+
| 0f8782af6a654d77b587e25a32f91f28 | cinder            |
| 1ab30f77400448eba6b2d47e55084540 | demo              |
| 2550fa93b1fe4cb582f1f46353b836d8 | ceilometer        |
| 2d2a345336184b1ebbdf022f710084e8 | neutron           |
| 48b816f9db9541b4bd9ca49ad453574c | glance            |
| 4c989a43a75c477bb4f9b7566cde6028 | userA             |
| 765a16c99d7d42a4b69ff941f7791b54 | aodh              |
| 788efa329f324b91a431ad56cd7b9a14 | nova              |
| 7ecae98d16d54483b964c9c2548fd7bc | swift             |
| 8a33fc3342154a3ca264ae7b918648ba | userB             |
| 962612a3e7784df38d0c98fea1f30320 | heat              |
| 9ee4731c00c24f659b8790be6b77bc8a | admin             |
| d6fdd1e5e1a348e0b6c5b8c7f33ba5fa | placement         |
| d957a578fed2452ab91bc651f2f1fb97 | heat_domain_admin |
| e91070fa751e49689963b566db999bee | gnocchi           |
+----------------------------------+-------------------+
```

可以看见租户与用户均已存在。

（2）上传镜像

使用cirros-0.3.4-x86_64-disk.img上传至控制节点的/root目录下，并上传至云平台中，命令如下：

```shell
[root@controller ~]# glance image-create --name cirros --disk-format qcow2 --container-format bare --progress ] 100%
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | ee1eca47dc88f4879d8a229cc70a07c6     |
| container_format | bare                                 |
| created_at       | 2022-02-10T05:31:48Z                 |
| disk_format      | qcow2                                |
| id               | 1fa9cbfe-392f-437e-ad18-f00987415b15 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros                               |
| owner            | 55b50cbb4dd4459b873cb15a8b03db43     |
| protected        | False                                |
| size             | 13287936                             |
| status           | active                               |
| tags             | []                                   |
| updated_at       | 2022-02-10T05:31:49Z                 |
| virtual_size     | None                                 |
| visibility       | shared                               |
+------------------+--------------------------------------+
```

上传镜像后，userA和userB都不能看到该镜像。接下来做相关配置，使得A租户中的用户可以看到该镜像。

（3）权限配置

首先将镜像共享给A租户，命令如下：

```shell
[root@controller ~]# glance member-create 1fa9cbfe-392f-437e-ad18-f00987415b15 df58511d2c914690b48e89f1e512ae6b
[root@controller ~]# glance member-create 镜像ID 项目ID
+--------------------------------------+----------------------------------+---------+
| Image ID                             | Member ID                        | Status  |
+--------------------------------------+----------------------------------+---------+
| 1fa9cbfe-392f-437e-ad18-f00987415b15 | df58511d2c914690b48e89f1e512ae6b | pending |
+--------------------------------------+----------------------------------+---------+
```

在共享之后，镜像的状态是pending状态，此时还需要激活镜像，命令如下：

```shell
[root@openstack ~]# glance member-update 1fa9cbfe-392f-437e-ad18-f00987415b15 df58511d2c914690b48e89f1e512ae6b accepted
+--------------------------------------+----------------------------------+----------+
| Image ID                             | Member ID                        | Status   |
+--------------------------------------+----------------------------------+----------+
| 1fa9cbfe-392f-437e-ad18-f00987415b15 | df58511d2c914690b48e89f1e512ae6b | accepted |
+--------------------------------------+----------------------------------+----------+
```

此时镜像的状态就变为了accepted，可以在dashboard界面登录userA用户，查看是否可以看到cirros镜像（也可以登录userB用户，查看是否能看到镜像），如图3-1所示：
![图31.png](私有云部署.assets/wKggBmIga4KAPahqAAAg5-sMDtE487.png)
图3-1 镜像界面

通过这种方式，可以使用管理员设置不同租户对不同镜像的访问权限







## openstack平台对接堡垒机

####  云堡垒机简介

堡垒机，即在一个特定的网络环境下，为了保障网络和数据不受来自外部和内部用户的入侵和破坏，而运用各种技术手段监控和记录运维人员对网络内的服务器、网络设备、安全设备、数据库等设备的操作行为，以便集中报警、及时处理及审计定责。

云堡垒机（Cloud Bastion Host，CBH）是一款4A统一安全管控平台，为企业提供集中的帐号（Account）、授权（Authorization）、认证（Authentication）和审计（Audit）管理服务。

云堡垒机是一种可提供高效运维、认证管理、访问控制、安全审计和报表分析功能的云安全服务。云租户运维人员可通过云堡垒机完成资产的运维和操作审计。堡垒机通过基于协议正向代理可实现对SSH、Windows远程桌面、SFTP等常见的运维协议的数据流进行全程记录，再通过数据流重置的方式进行录像回放，达到运维审计的目的。

云堡垒机提供云计算安全管控的系统和组件，包含部门、用户、资源、策略、运维、审计等功能模块，集单点登录、统一资产管理、多终端访问协议、文件传输、会话协同等功能于一体。通过统一运维登录入口，基于协议正向代理技术和远程访问隔离技术，实现对服务器、云主机、数据库、应用系统等云上资源的集中管理和运维审计。

云堡垒机无需安装部署，可通过HTML5技术连接管理多个云服务器，企业用户只需使用主流浏览器或手机APP，即可随时随地实现高效运维。云堡垒机支持RDP/SSH/Telnet/VNC等多种协议，可访问所有Windows、Linux/Unix操作系统。企业用户可以通过云堡垒机管理多台云服务器，满足等保三级对用户身份鉴别、访问控制、安全审计等条款的要求。

其从功能上讲，它综合了核心系统运维和安全审计管控两大主干功能，从技术实现上讲，通过切断终端计算机对网络和服务器资源的直接访问，而采用协议代理的方式，接管了终端计算机对网络和服务器的访问。形象地说，终端计算机对目标的访问，均需要经过运维安全审计的翻译。打一个比方，运维安全审计扮演着看门者的工作，所有对网络设备和服务器的请求都要从这扇大门经过。因此运维安全审计能够拦截非法访问和恶意攻击，对不合法命令进行命令阻断，过滤掉所有对目标设备的非法访问行为，并对内部人员误操作和非法操作进行审计监控，以便事后责任追踪。

传统堡垒机多以硬件形式进行售卖，硬件一体机本质上就是将软件部署在独立的硬件设备之上。尽管硬件一体机在部署上线和独立运维上有其优势，但在面临新一代堡垒机需要解决的各种需求时越来越成为一种限制。同时，硬件一体机带来的额外硬件维护管理工作也成为运维人员的一种负担。随着硬件虚拟化技术及云平台的普及，软件部署方式越来越成为堡垒机的首选部署方式。因此，相较于硬件而言，软件模式不仅更易于部署和维护，还在扩缩容、高可用方案上更具灵活的优势。如图4-1所示：

![null](私有云部署.assets/wKggBmIga8qAW3c6AAkv3V9NHNE741.png)

图4-1 堡垒机

------

#### 云堡垒机优势

（1）HTML5一站式管理

无需安装特定客户端，无需安装任何插件，任意终端的主流浏览器，包括移动端APP浏览器登录，用户随时随地打开即可进行运维。

系统HTML5管理界面简洁易用，集中管理用户、资源和权限，支持批量创建用户、批量导入资源、批量授权运维、批量登录资源等高效运维管理方式。

（2）操作指令精准拦截

针对资源敏感操作进行二次复核，系统预置标准Linux字符命令库或自定义命令，对运维操作指令和脚本的精准拦截，并可通过异步“动态授权”，实现对敏感操作的动态管控，防止误操作或恶意操作的发生。

（3）核心资源二次授权

借鉴银行金库授权机制，针对重要资源的运维权限设置多人授权，若需登录此类资源，需多位授权候选人进行“二次授权”，加强对核心资源数据的保护，提升数据安全防护能力和管理能力，保障核心资产数据的绝对安全。

（4）应用发布扩展

针对数据库类、Web应用类、客户端程序类等不同应用资源，提供统一访问入口，并可提高对应用操作的图形化审计。

（5）数据库运维审计

针对DB2、MySQL、SQL Server和Oracle等云数据库，支持统一资源运维管理，以及SSO单点登录工具一键登录数据库，提供对数据库操作的全程记录，实现对云数据库的操作指令进行解析，100%还原操作指令。

（6）自动化运维

自动化运维是将系统运维管理中复杂的、重复的、数量基数大的操作，通过统一的策略、任务将复杂运维精准化和效率化，帮助运维人员从重复的体力劳动中解放出来，提高运维效率。

------

#### 部署堡垒机

（1）修改主机名

远程连接堡垒机节点，修改节点的主机名为jumpserver。如下所示：

```shell
[root@jumpserver ~]# hostnamectl set-hostname jumpserver
```

（2）关闭防火墙与SELinux

将节点的防火墙与SELinux关闭，并设置永久关闭SELinux，命令如下：

```shell
[root@jumpserver ~]# setenforce 0
[root@jumpserver ~]# sed  -i  s#SELINUX=enforcing#SELINUX=disabled#   /etc/selinux/config
[root@jumpserver ~]# iptables -F
[root@jumpserver ~]# iptables -X
[root@jumpserver ~]# iptables -Z
[root@jumpserver ~]# /usr/sbin/iptables-save
```

（3）配置本地Yum源

使用提供的软件包配置Yum源，使用远程连接工具自带的传输工具，将jumpserver.tar.gz软件包上传至jumpserver节点的/root目录下。

解压软件包jumpserver.tar.gz至/root目录下，命令如下：

```shell
[root@jumpserver ~]# tar -zxvf jumpserver.tar.gz -C /opt/
[root@jumpserver ~]# ls /opt/
compose  config  docker  docker.service  images  jumpserver-repo  static.env
```

将默认Yum源移至其他目录，创建本地Yum源文件，命令及文件内容如下：

```shell
[root@jumpserver ~]# mv /etc/yum.repos.d/* /media/
[root@jumpserver ~]# cat >> /etc/yum.repos.d/jumpserver.repo << EOF
[jumpserver]
name=jumpserver
baseurl=file:///opt/jumpserver-repo
gpgcheck=0
enabled=1
EOF
[root@jumpserver ~]# yum repolist
repo id		repo name		status
jumpserver	jumpserver		2
```

（4）安装依赖环境

安装python数据库，命令如下：

```shell
[root@jumpserver ~]# yum install python2 -y
```

安装配置docker环境，命令如下：

```shell
[root@jumpserver ~]# cp -rf /opt/docker/* /usr/bin/
[root@jumpserver ~]# chmod 775 /usr/bin/docker*
[root@jumpserver ~]# cp -rf /opt/docker.service /etc/systemd/system/
[root@jumpserver ~]# chmod 755 /etc/systemd/system/docker.service
[root@jumpserver ~]# systemctl daemon-reload
[root@jumpserver ~]# systemctl enable docker --now
```

验证服务状态，命令如下：

```shell
[root@jumpserver ~]# docker --version
Docker version 18.06.3-ce, build d7080c1
[root@jumpserver ~]# docker-compose --version
docker-compose version 1.27.4, build 40524192
```

（5）安装jumpserver服务

加载jumpserver服务组件镜像，命令如下：

```shell
[root@jumpserver ~]# cd /opt/images/
[root@jumpserver images]# sh load.sh
```

创建jumpser服务组件目录，命令如下：

```shell
[root@jumpserver images]# mkdir -p /opt/jumpserver/{core,koko,lion,mysql,nginx,redis}
[root@jumpserver images]# cp -rf /opt/config /opt/jumpserver/
```

生效环境变量static.env，使用所提供的脚本up.sh启动jumpserver服务，命令如下：

```shell
[root@jumpserver images]# cd /opt/compose/
[root@jumpserver compose]# source /opt/static.env
[root@jumpserver compose]# sh up.sh                  
Creating network "jms_net" with driver "bridge"
Creating jms_mysql ... done
Creating jms_redis ... done
Creating jms_core  ... done
Creating jms_celery ... done
Creating jms_luna   ... done
Creating jms_lion   ... done
Creating jms_lina   ... done
Creating jms_nginx  ... done
Creating jms_koko   ... done
```

浏览器访问http://192.168.100.102，jumpserver web登录（admin/admin），如图4-2所示：

![null](私有云部署.assets/wKggBmIga9eAT39nAAAePB9It8g679.png)

图4-2 web登录

设置新密码，如图4-3、图4-4所示：

![null](私有云部署.assets/wKggBmIga-qAfnBZAAAhTvKSf7A214.png)

图4-3 修改密码

![null](私有云部署.assets/wKggBmIga_OAd7leAADTcjkSsrY560.png)

图4-4 登录成功

至此jumpserver安装完成。

（6）管理资产

使用管理员admin用户登录jumpserver管理平台，点击左侧导航栏，展开“资产管理”项目，选择“管理用户”，点击右侧“创建”按钮，如图4-5所示：

![null](私有云部署.assets/wKggBmIga_yAZOWmAAFvjE7VkVA553.png)

图4-5 管理用户

创建远程连接用户，用户名为root密码为服务器密码，点击提交进行创建，如图4-6所示：

![null](私有云部署.assets/wKggBmIgbAaARPcQAACF3aZmmH0738.png)

图4-6 创建管理用户

选择“系统用户”，点击右侧“创建”按钮，创建系统用户，选择主机协议“SSH”，设置密码为服务器ssh密码，如图4-7所示：

![null](私有云部署.assets/wKggBmIgbA-ADZNrAABgxD7RKHk937.png)

图4-7 创建系统用户

点击左侧导航栏，展开“资产管理”项目，选择“资产列表”，点击右侧“创建”按钮，如图4-8所示：

![null](私有云部署.assets/wKggBmIgbBeAXMYBAAFoZqEgoiY321.png)

图4-8 管理资产

创建资产，将云平台主机（controller）加入资产内，如图4-9所示：

![null](私有云部署.assets/wKggBmIgbB6AU1ltAABefrdWv9g600.png)

图4-9 创建资产controller

创建资产，将云平台主机（compute）加入资产内，如图4-10、图4-11所示：

![null](私有云部署.assets/wKggBmIgbCWATbrmAABZow0PQ7k277.png)

图4-10 创建资产compute

![null](私有云部署.assets/wKggBmIgbC2AIkdWAABiJXIlFsE886.png)

图4-11 创建成功

（7）资产授权

点击左侧导航栏，展开“权限管理”项目，选择“资产授权”，点击右侧“创建”按钮，创建资产授权规则，如图4-12所示：

![null](私有云部署.assets/wKggBmIgbDSAOvd-AABV-qCy_mw776.png)

图4-12 创建资产授权规则

（8）测试连接

点击右上角管理员用户下拉箭头，选择“用户界面”，如图4-13所示：

![null](私有云部署.assets/wKggBmIgbDuASScpAAAq_bs5GlA699.png)

图4-13 创建资产授权规则

如果未出现Default项目下两个资产主机，点击收藏夹后刷新按钮进行刷新，如图4-14所示：

![null](私有云部署.assets/wKggBmIgbESAFYDKAABhQHv9Jbc253.png)

图4-14 查看资产

点击左侧导航栏，选择“Web终端”进入远程连接页面，如图4-15所示：

![null](私有云部署.assets/wKggBmIgbEuAR5tQAACKv3Q2w8w443.png)

图4-15 进入远程连接终端

点击左侧Default，展开文件夹，点击controller主机，右侧成功连接主机，如图4-16所示：

![null](私有云部署.assets/wKggBmIgbGWAb8XRAADogqPUt9w683.png)

图4-16 测试连接

至此OpenStack对接堡垒机案例实操成功。







# 系统调优



## 数据库调优

```
# vim /etc/my.cnf

#数据库支持大小写
lower_case_table_names =1
#数据库缓存
innodb_buffer_pool_size = 4G
#数据库的log buffer即redo日志缓冲
innodb_log_buffer_size = 64MB
#设置数据库的redo log即redo日志大小
innodb_log_file_size = 256MB
#数据库的redo log文件组即redo日志的个数配置
innodb_log_files_in_group = 2

# 重启服务
systemctl restart mariadb

# mysql -uroot -p000000 -e " show variables like 'innodb_log%';"
```



## dashboard优化

```
#根据题意找线索，是关于django的数据，这个单词他写错了，就检索出来所有关于他的配置
#会发现有一行
#SESSION_ENGINE = 'django.contrib.sessions.backends.cache'存在cache里，改一改就行。

cat /etc/openstack-dashboard/local_settings |grep django
SESSION_ENGINE = 'django.contrib.sessions.backends.file'

#重启服务生效配置
systemctl restart httpd

```









## 句柄优化

####  Linux系统句柄介绍

文件句柄，会随着进程数增加而增加。其实Linux是有文件句柄限制的，而且Linux默认一般都是1024。在生产环境中很容易到达这个值，因此这里就会成为系统的瓶颈。

在Linux系统的生产环境中，会经常遇到“too many open files”的报错。这个报错顾名思义是打开过多文件数。不过这里的files不单是文件的意思，也包括打开的通讯链接（比如socket），正在监听的端口等等，所以有时候也可以叫做句柄（handle），这个错误通常也可以叫做句柄数超出系统限制。

在出现“too many open files”报错的时候，大多数情况是由于程序没有正常关闭一些资源引起的，所以出现这种情况，这个时候需要检查I/O读写，socket通讯等是否正常关闭等。当然也可以通过修改参数，将系统的文件句柄限制提高，来缓解这一压力。

------

####  Linux系统句柄优化

（1）查看当前的句柄数

使用CRT等远程连接工具，连接至192.168.200.21，查看当前系统默认的文件句柄数量，命令如下：

```shell
[root@test ~]# ulimit -n
1024
```

可以看到当前的系统默认限制是1024。

（2）修改句柄数.

修改Linux系统的句柄数限制有两种方法，一种是使用ulimit命令临时生效，另外一种是修改配置文件，永久生效。此处使用修改配置文件的方式。

**注意：**如果使用命令临时生效句柄限制，root用户可以调大或者调小句柄的限制，而非root用户一旦设定了值，就只能调小这个限制，不能调大，不然会报“bash: ulimit: open files: cannot modify limit: Operation not permitted”的错误。

修改配置文件调整句柄限制为65535，命令如下：

```shell
[root@test ~]# echo "* soft nofile 65535"  >> /etc/security/limits.conf
[root@test ~]# echo "* hard nofile 65535"  >> /etc/security/limits.conf
```

添加配置的解释如下：

```shell
*       #代表所有用户
soft    #超过文件句柄数时，仅提示
hard    #超过文件句柄数时，直接限制
```

修改完之后，不需要重启系统即会生效，但是需要重新登录。退出重新登录该云主机，查看当前的句柄限制，命令如下：

```shell
[root@test ~]# logout

Connection closed.
Last login: Tue Feb  8 01:58:31 2022 from 192.168.0.70
[root@test ~]# ulimit -n
65535
```

可以看到当前的限制为65535



##  Nova调度策略优化

####  OpenStack平台报错分析

在OpenStack平台经历大并发的时候，比如同一个平台，大量的用户同时创建云主机（单个用户创建大量云主机不会触发此种现象），会达到云平台的性能瓶颈，导致创建云主机报错。

大量用户同时创建云主机，会对云平台的两个服务造成性能瓶颈，一个是RabbitMQ，当RabbitMQ达到瓶颈时，会报如下错误：

```shell
ERROR oslo.messaging._drivers.impl_rabbit [req-eb79ea09-247e-49e0-960b-0896ef978661 - - - - -] [303415c0-e494-4ea2-8158-d66d4165600d] AMQP server on controller:5672 is unreachable: timed out. Trying again in 10 seconds.: timeout: timed out
```

另一个就是DHCP。本案例重点讨论DHCP报错的情况，不考虑RabbitMQ的性能瓶颈。DHCP报错信息如下：

```shell
WARNING nova.compute.manager [req-8d9240cd-6a47-4979-a289-bdd58d399f0a 891c061e4aea4af8909a4affe0c24f92 c509a52800de4902845460fcc5318f3f - 8d899afee33641e0a094f85fbeb9b2c6 8d899afee33641e0a094f85fbeb9b2c6] [instance: 374aa944-6cd7-4dbf-a741-5bd44623919d] Received unexpected event network-vif-plugged-b803941a-c3ae-45d7-b962-13ba1ff00a31 for instance with vm_state active and task_state None.
```

在这种情况下，因为大量的创建云主机，导致获取IP地址超时，然后就发生了如上的报错。

**注意：这个报错还和计算节点的性能有关，大量创建云主机的时候，计算节点如果没有创建过该镜像的云主机，会先从控制节点复制镜像到计算节点，这也会导致速度变慢。而且在创建大量云主机的时候，对计算节点的硬盘也是一个考验，如果硬盘性能差的话，也会导致创建速度慢，大量排队，超时等现象。**

------

#### 解决策略

在解决问题之前，首先了解创建云主机的过程，在创建虚机过程中，nova-compute会调用wait_for_instance_event函数（nova/compute/manage.py）进行network-vif-plugged的事件等待。

在nova-compute配置文件中有两个与该事件相关的参数- vif_plugging_timeout、vif_plugging_if_fatal，前者是等待事件的最大时间，后者是处理超时异常的方式。若在规定时间内，nova-compute接受到了事件响应，那么虚机可正常创建，那么当超时现象发生时，nova-compute会根据vif_plugging_is_fatal的配置采取两种处理方式。

若超时发生，并且配置文件中vif_plugging_is_fatal为True，nova首先执行guest.poweroff，停止qemu进程；然后执行cleanup函数，先清除网络资源，使用函数_unplug_vifs，删除plug_vifs函数创建的ovs port，ovs agent检测到了删除端口的事件然后通知neutron-server删除neutron db中的port信息，最后抛VirtualInterfaceCreateException。

如果vif_plugging_is_fatal为False，即便发生eventlet.timeout.Timeout异常，创建过程也会继续。然后等虚拟机创建成功后，依然可以拿到IP地址。

通过上述虚拟机创建的过程，对于上述云平台发生的错误，就有了解决的思路，通过修改/etc/nova/nova.conf配置文件，将vif_plugging_is_fatal参数由true改为false，命令如下：

在控制节点，修改/etc/nova/nova.conf配置文件：

```shell
[root@controller ~]# vi /etc/nova/nova.conf
```

找到如下这行

```
#vif_plugging_is_fatal=true
```

将该行的注释去掉，并将true改为false，修改完之后如下：

```
vif_plugging_is_fatal=false
```

保存退出nova.conf，最后重启nova服务，也可以重启所有服务，命令如下：

```shell
[root@controller ~]# openstack-service restart

# 重启nova-*
systemctl restart openstack-nova*
```

等待重启完毕即可。通过该参数的修改，可解决在大并发量创建虚拟机时，因排队超时导致虚拟机获取不到IP地址的报错。







## 镜像优化

在使用打快照方式制作镜像后，镜像的大小会变得非常大，比如一个基础的CentOS镜像大小为400M左右，但是使用打快照方式制作的镜像大小会有1个G左右，具体的大小还要根据安装的东西来实际情况实际分析。

qemu-img命令中提供一个可用于镜像转换与压缩的选项，即qemu-img convert。接下来使用该命令，对已经打快照完成的镜像进行压缩操作。

使用提供的镜像CentOS7.5-compress.qcow2，上传至controller节点的/root目录下，查看镜像的大小，命令如下：

```shell
[root@controller ~]# du -sh CentOS7.5-compress.qcow2
892M	CentOS7.5-compress.qcow2
```

可以看到当前的镜像大小为892M，接下来使用命令，对镜像进行压缩，命令如下：

```shell
[root@controller ~]# qemu-img convert -c -O qcow2 CentOS7.5-compress.qcow2 CentOS7.5-compress2.qcow2
```

该命令参数的解释如下：

```shell
-c  压缩
-O  qcow2 输出格式为 qcow2
CentOS7.5-compress.qcow2   被压缩的文件
CentOS7.5-compress2.qcow2  压缩完成后文件
```

等待一小段时间后，压缩完成，查看当前目录下CentOS7.5-compress2.qcow2镜像文件的大小，命令如下：

```shell
[root@controller ~]# du -sh CentOS7.5-compress2.qcow2 
405M	CentOS7.5-compress2.qcow2
```

可以看到镜像大概被压缩到了一半的大小。使用qemu-img convert命令可以压缩qcow2镜像，在日常的工作中，经常会用到此命令进行镜像压缩。







## I/O优化

为什么需要IO调度呢？在最开始的时候，Linux存储在磁盘上。磁盘盘片高速旋转，通过磁臂的移动读取数据。磁臂的移动是物理上的机械上的移动，它无法瞬移，这速度是很慢的。如果我们读取的数据位置很随机，一会在A地点，一会在隔着老远的B地点，移动的时间就全做了无用功，这也就是我们说的随机读写性能慢的原因。如果读取的数据地址是连续的，即使不是连续的也是地址接近的，那么移动磁臂的时间损耗就少了。在最开始，IO调度的作用就是为了合并相近的IO请求，减少磁臂的移动损耗。

####  单队列I/O调度介绍

登录物理OpenStack平台的Controller节点，查看I/O调度策略，命令如下：

```shell
[root@controller ~]# cat /sys/block/sda/queue/scheduler
noop [deadline] cfq 
```

可以看到，物理节点默认使用的是deadline算法。常用的单队列算法就是noop、deadline和cfq，关于这三种调度算法的详细解释如下：

（1）noop

noop只会对请求做一些简单的排序，其本质就是一个FIFO的队列，只会简单地合并临近的I/O请求后，本质还是按先来先处理的原则提交给磁盘。

根据它的原理，我们可以发现它倾向于饿死读利于写，为什么呢？异步写是把数据直接放到page cache的，也就意味着可以通过page cache缓存大量的写数据，再一次性往下提交I/O请求。而读呢？读一般是同步的，也就意味着必须在读完一笔后再读下一笔，两次读之间是可能被写请求插足的。

（2）cfq

CFQ全称Completely Fair Scheduler，中文名称完全公平调度器，它是现在许多Linux发行版的默认调度器，CFQ是内核默认选择的I/O调度器。它将由进程提交的同步请求放到多个进程队列中，然后为每个队列分配时间片以访问磁盘。对于通用的服务器是最好的选择，CFQ均匀地分布对I/O带宽的访问。CFQ为每个进程和线程，单独创建一个队列来管理该进程所产生的请求,以此来保证每个进程都能被很好的分配到I/O带宽，I/O调度器每次执行一个进程的4次请求。该算法的特点是按照I/O请求的地址进行排序，而不是按照先来后到的顺序来进行响应。简单来说就是给所有同步进程分配时间片，然后才排队访问磁盘。

（3）deadline

deadline确保请求在一个用户可配置的时间内得到响应，避免请求饿死。其分别为读I/O和写I/O提供不同的FIFO队列，读FIFO队列的最大等待时间是500ms，写FIFO队列的最大等待时间是5s。deadline会把提交时间相近的请求放在一批。在同一批中，请求会被排序。当一批请求达到了大小上限或着定时器超时，这批请求就会提交到设备队列上去。

总体来讲，deadline算法对request进行了优先权控制调度，主要表现在如下几个方面：

读写请求分离，读请求具有高优先调度权，除非写请求即将被饿死的时候，才会去调度处理写请求。这种处理可以保证读请求的延迟时间最小化。

对请求的顺序批量处理。对那些地址临近的顺序化请求，deadline给予了高优先级处理权。例如一个写请求得到调度后，其临近的request会在紧接着的调度过程中被处理掉。这种顺序批量处理的方法可以最大程度的减少磁盘抖动。

保证每个请求的延迟时间。每个请求都赋予了一个最大延迟时间，如果达到延迟时间的上限，那么这个请求就会被提前处理掉，此时，会破坏磁盘访问的顺序化特征，回影响性能，但是，保证了每个请求的最大延迟时间。

对于这三种调度算法的总结如下：

● noop

对于闪存设备和嵌入式系统是最好的选择。对于固态硬盘来说使用noop是最好的，deadline次之，而CFQ效率最低。

● cfq

为所有进程分配等量的带宽,适用于有大量进程的多用户系统，CFQ是一种比较通用的调度算法，它是一种以进程为出发点考虑的调度算法，保证大家尽量公平,为所有进程分配等量的带宽,适合于桌面多任务及多媒体应用。

● deadline

适用于大多数环境,特别是写入较多的文件服务器，从原理上看，DeadLine是一种以提高机械硬盘吞吐量为思考出发点的调度算法，尽量保证在有I/O请求达到最终期限的时候进行调度，非常适合业务比较单一并且I/O压力比较重的业务，比如Web服务器，数据库应用等。

------

#### 多队列I/O调度介绍

现在有多种multi-queue（多队列）调度器，分别为bfq，none，kyber和mq-deadline。下面对常用调度器的进行介绍：

（1）mq-deadline

mq-deadline调度器跟单队列的deadline调度器发挥的功能很相似。它有个insert_request()函数，不会使用多个staging队列，而是把请求放到两个全局的基于时间的队列中，一个放读请求，一个放写请求，先尝试把该新请求与已经存在的请求合并，如果合并不了，则把这个新请求放到队列尾部。dispatch_request()函数会从这些队列中返回第一个请求：基于时间的队列，基于请求批大小，以及避免写饥饿的队列。

（2）none

多队列无操作I/O调度程序。不对请求进行重新排序，最小的开销。NVME等快速随机I/O设备的理想选择。

------

####  I/O调度策略修改

调度策略的修改是比较简单的，首先查看当前使用的调度算法，使用CRT工具连接到controller节点，查看调度算法，命令如下：

```shell
[root@controller ~]# cat /sys/block/vda/queue/scheduler 
[mq-deadline] kyber none
```

可以看到当前的I/O调度算法为mq-deadline，如果当前全是用的是SSD硬盘，那么显然none算法更合适，修改算法为none，命令如下：

```shell
[root@test ~]# echo none > /sys/block/vda/queue/scheduler
```

修改完之后，查看当前使用的算法，命令如下：

```shell
[root@test ~]# cat /sys/block/vda/queue/scheduler 
[none] mq-deadline kyber 
```

可以看到当前的I/O调度算法为none模式。

以上就是I/O策略的优化，无论是单队列还是多队列，都可以使用echo命令去修改当前的I/O策略。选择何种策略，也需要根据当前使用的硬盘与应用场景来决定，不能盲目修改。





## 内存优化

####  Linux内存大页介绍

随着业务和计算机硬件技术不断发展，出现越来越大的内存机器（≥700G），满足复杂计算处理需求，同时Linux内核的同步持续迭代升级，操作系统可以支持现代硬件架构的大页面容量功能，以适应越来越大的系统内存。

在云上大规模机器集群中，传统内存页面管理机制存在一定的性能问题，以典型推荐系统服务为例，一方面单次请求产生的中间数据规模较大，另一方面程序广泛应用了本地词典&缓存等技术，这使响应单次请求所需要涉及的内存访存范围显著增大。通过perf统计线上部分典型程序的dtlb_load_misses.walk_active和dtlb_store_misses.walk_active的占比，也观测到页表缓存命中率不足的表现，平均观测大多程序有5%-10%的时间周期存在未命中导致陷入Page Walk的现象。

目前在机器硬件大内存和系统内核技术都具备情况下，透明大页（Transparent Huge Page）内存管理应用成为可能。通过增大页面大小，预期可以显著解决页表缓存命中不足的现象，进而普遍提升程序性能。但是由于存在若干缺陷，主要集中在内存用量显著上涨、频繁回收重整带来额外开销增加以及不可控的随机卡顿。基于这些因素，目前业界对透明大页机制的一般认知均偏负向，排查问题追到透明大页，尚无确切结论说明对服务影响情况，这也是应用透明大页技术一道难题。

####  内存大页技术演进

内存作为计算机上非常重要的一种资源，程序加载运行、CPU指令和数据获取等都依赖内存使用，因此内存访问性能是影响计算机性能的一个很重要的因素。现代Linux系统上内存的分配主要过程如下：

（1）应用程序通过调用内存分配函数（malloc, free, realloc, calloc），系统调用brk或者mmap进行内存分配，申请虚拟内存地址空间。

（2）虚拟内存至物理内存映射处理过程，通过请求MMU分配单元，根据虚拟地址计算出该地址所属的页面，再根据页面映射表的起始地址计算出该页面映射表（PageTable）项所在的物理地址，根据物理地址在高速缓存的TLB中寻找该表项的内容，如果该表项不在TLB中，就从内存将其内容装载到TLB中。包含如下：

- **虚拟内存（Virtual Memory）**：现代操作系统普遍使用的一种技术，每个进程有用独立的逻辑地址空间，内存被分为大小相等的多个块,称为页（Page）。每个页都是一段连续的地址，对应物理内存上的一块称为页框，通常页和页框大小相等。
- **MMU（Memory-Management Unit）**：内存管理单元，负责管理虚拟地址到物理地址的内存映射，实现各个用户进程都拥有自己的独立的地址空间，提供硬件机制的内存访问权限检查，保护每个进程所用的内存不会被其他的进程所破坏。
- **PageTable：** 虚拟内存至物理内存页面映射关系存储单元。
- **TLB（Translation Lookaside Buffer）：**高速虚拟地址映射缓存, 主要为了提升MMU地址映射处理效率，加了缓存机制，如果存在即可直接取出映射地址供使用。

####  标准大页(HugePage)

标准大页Huge pages 通过提升单位页面的大小，替代传统的 4 KB 内存页面，以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能，降低MMU管理开销，提升内存处理性能。主要有两种格式大小——2MB 和1GB ，2MB块大小适合GB 大小的内存， 1GB 页块大小适合用于 TB 级别的内存；2 MB 是默认的页大小。

**优点：**

- 无需交换：不存在页面由于内存空间不足而换入换出的问题。
- 减轻 TLB Cache 的压力：相同的内存大小，管理的虚拟地址数量变少，降低了 CPU Cache 可缓存的地址映射压力。
- 降低 Page Table 负载：内存页的数量会减少，从而需要更少的Page Table，节约了页表所占用的内存数量。
- 消除Page Table查找负载：因为页面不需要替换，所以不需要页表查找。
- 提高内存的整体性能：页面变大后，处理的页面较少，因此可以明显避免访问页表时可能出现的瓶颈。

**缺点：**

- 需要合理设置，避免内存浪费：在操作系统启动期间被动态分配并被保留，共享内存不会被置换，在使用HugePage的内存不能被其他的进程使用，所以要合理设置该值，避免造成内存浪费。
- 静态设置无法自适应：如果增加HugePage或添加物理内存或者是当前服务器增加了新的实例发生变化，需要重新设置所需的HugePage大小。

**使用方式：**

（1）标准大页查看机器开启情况

可以通过以下命令查看大页开启情况：

```shell
# grep Huge /proc/meminfo
AnonHugePages:   10240 kB
HugePages_Total:    0
HugePages_Free:     0
HugePages_Rsvd:     0
HugePages_Surp:     0
Hugepagesize:    2048 kB
```

（2）大页设置

通过设置 vm.nr_hugepages 参数的值修改大页数量，大页分配需要连续的内存，机器长期运行存在内存碎片，可能不满足分配需要，一般开机启动时就需要分配设置，如使用以下命令设置大页配置：

```shell
[root@controller ~]# sysctl -w vm.nr_hugepages=20
```

（3）大页使用

应用需要通过mmap系统调用或者shmat和shmget系统调用适配才可以使用大页，一般的应用进程都是不可以访问此大页空间。

#### 透明大页（Transparent Huge Pages）

标准大页可带来性能提升，但存在配置和管理上的困难，很难手动管理，而且通常不能通用适配应用的接入使用，需要对代码进行重大的更改才能有效使用。从RHEL 6 开始引入透明大页技术（kernel ≥kernel-2.6.32），在标准大页的基础上，通过一个抽象层，能够自动创建、管理和使用传统大页，用来提高内存管理的性能，解决大页手动管理的问题，透明大页为系统管理员和开发人员减少了很多使用传统大页的复杂性。

**优点：**

- 相对标准大页，操作系统动态管理大页分配，支持大页分配 ，大页→普通页换出拆分，根据具体应用使用情况，自适应调整分配。
- 使用上对应用透明，且开源社区持续测试、优化和适配大部分应用，同时支持多种大页申请方式配置，满足多场景的分配需求。

**缺点：**

- THP在运行时动态分配内存，可能会带来运行时内存分配的CPU开销和上涨问题。
- THP同步申请模式下，分配失败时触发申请时，导致应用的一些随机卡顿现象。

**使用方式：**

透明大页生效方式：对匿名内存的透明Hugepage支持可以完全禁用，也可以仅在MADV_HUGEPAGE区域内启用（避免消耗更多内存资源的风险），也可以在系统范围内启用，可以通过以下方法之一实现：

```shell
# echo always >/sys/kernel/mm/transparent_hugepage/enabled
# echo madvise >/sys/kernel/mm/transparent_hugepage/enabled
# echo never >/sys/kernel/mm/transparent_hugepage/enabled
```

内存大页整理模式：

| 模式    | 功能说明                                                     |
| ------- | ------------------------------------------------------------ |
| always  | 同步申请，分配失败时，直接回收页面和压缩内存，以便立即分配THP |
| madvise | 使用madvise（MADV HUGEPAGE）的区域，直接回收和压缩内存，提供THP申请 |
| never   | 关闭内存大页                                                 |

启用透明大页命令如下：

```shell
# echo always >/sys/kernel/mm/transparent_hugepage/enabled
# echo always >/sys/kernel/mm/transparent_hugepage/defrag
```

关闭透明大页命令如下：

```shell
# echo never >/sys/kernel/mm/transparent_hugepage/enabled
# echo never >/sys/kernel/mm/transparent_hugepage/defrag
```





## SYN攻击调优

#### 1. SYN攻击简介

所谓SYN攻击，即利用TCP三次握手原理，向服务器发送大量的SYN数据包，却不响应服务器反馈的SYN+ACK数据包，导致服务器的网络、内存等资源被大量占用，从而导致正常用户无法访问，起到了拒绝服务攻击的目的。

如果想要杜绝SYN攻击，那么我们应该设置服务器减少对ACK数据包的等待时间，减少对SYN+ACK数据包的重发次数，增大网络连接队列的长度等等，

防止SYN攻击的相关内核参数如下所示：

（1）net.ipv4.tcp_synack_retries

该参数表示服务器在收到客户端的SYN数据包并发送SYN+ACK数据包后，如果没有收到客户端的ACK数据包的重发SYN+ACK数据包的次数。在默认情况下，该值设置为5，即表示服务器会尝试发送5次SYN+ACK数据包，如果依旧没有收到响应，则中断链接，不再进行尝试。

（2）net.ipv4.tcp_max_syn_backlog

该参数表示SYN队列的长度，默认为128，可以将该参数设置为20480，以容纳更多的网络链接数。

（3）fs.file-max

该参数表示系统允许的文件句柄的最大数目，TCP连接的建立需要占用文件句柄，因此可以将该参数设置的大一点以应对大量的TCP链接请求。

（4）net.core.somaxconn

该参数表示socket监听队列长度，系统在默认情况下会将已经收到但是还没有处理的request请求放入到该队列中，当该队列满后就无法处理其他的request请求。因此增大该队列的长度可以应对更多的TCP连接请求。但是也会消耗系统的内存。

（5）net.core.wmem_max

该参数表示最大的TCP数据发送缓存，单位为字节。

（6）net.core.netdev_max_backlog

该参数表示当网络设备接收数据包的速率大于内核处理数据包速率时，允许将数据包送入队列的最大数目。

（7）net.ipv4.ip_local_port_range

该参数表示本机主动连接其他设备时的端口分配范围，通常设置为10000-65535，防止占用知名端口。

（8）net.ipv4.tcp_syncookies

该参数默认为0，表示关闭SYN Cookies，可以将该参数设置为1，表示开启该功能。SYN Cookies表示当出现SYN等待队列溢出时，启用Cookies来处理，可以防范少量的SYN攻击。

（9）net.ipv4.tcp_tw_reuse

该参数如果设置为1，则表示允许将TIME-WAIT sockets重新应用于新的TCP连接，如果设置为0，则表示关闭该功能。在默认情况下，该参数设置为0。

（10）net.ipv4.tcp_syn_retries

表示当该设备向其他设备发送TCP SYN包尝试建立TCP连接时，如果对方不响应（有可能是网络丢包或者是对方服务忙），则重新发送TCP SYN数据包的次数。

（11）net.ipv4.tcp_fin_timeout

该参数表示对于本端断开的socket连接，TCP保持在FIN-WAIT-2状态的时间，默认为60。

（12）net.ipv4.tcp_tw_recycle

该参数表示是否开启TCP连接中TIME-WAIT sockets快速回收，在默认情况下设置为0，表示不回收，如果设置为1，则表示回收。

#### 解决策略

（1）开启SYN cookie，防止SYN洪水攻击，修改配置文件/etc/sysctl.conf。

配置文件添加参数，如下所示：

```shell
# vi /etc/sysctl.conf
net.ipv4.tcp_syncookies = 1
```

生效修改配置，命令如下：

```SHELL
[root@controller ~]# sysctl -p
net.ipv4.tcp_syncookies = 1
```

（2）允许将TIME-WAIT sockets重新用于新的TCP连接；开启TCP连接中TIME-WAIT sockets的快速回收；修改系統默认的TIMEOUT时间为30。如下所示：

```SHELL
# vi /etc/sysctl.conf
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
```

生效修改配置，命令如下：

```SHELL
# sysctl -p     
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_fin_timeout = 30
```



## 配置文件

```
# vim /etc/nova/nova.conf
修改#virt_type=kvm
#virt_type=qcmu

systemctl restart openstack-nova-compute或systemctl restart openstack-nova-computels
```



# 参数调优

## Nova关键参数调优

```
# vim /etc/nova/nova.conf
# 预留前2个物理CPU，把后面的所有CPU分配给虚拟机使用（假设vcpu为16个）
vcpu_pin_set=2-15

# 设置cpu超售比例为4倍
cpu_allocation_ratio=4.0
# 设置内存超售比例为1.5倍
ram_allocation_ratio=1.5

# 预留2048mb内存，这部分内存不能被虚拟机使用
reserved_host_memory_mb=2048
# 预留1024mb硬盘，这部分不能被使用
reserved_host_disk_mb=1024

# 设置nova服务心跳检查时间为120秒
service_down_time=120
# 设置超时时间
rpc_response_timeout=300 RPC
最大返回数据长度限制
osapi_max_limit = 5000
```

## Keyston关键参数调优

## Glance关键参数调优

```
# 处理请求的子进程数量，如果为0则只有一个主进程
workers = 2 glance-api
# 最大返回数据长度限制
api_limit_max = 1000
# 一个响应中的最大返回项数
limit_param_default=1000
```





# Heat编排部署
